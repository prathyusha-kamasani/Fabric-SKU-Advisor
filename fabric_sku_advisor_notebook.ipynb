{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fabric SKU Advisor\n",
        "> **Version 1.1.0** | Updated: 2026-02-06 | [Data Nova](https://www.data-nova.io)\n",
        "## Throttling & carryforward analysis\n",
        "\n",
        "**By [Prathy Kamasani](https://www.linkedin.com/in/prathy/) | [Data Nova](https://www.data-nova.io)**\n",
        "\n",
        "*Microsoft Fabric Training & Consulting*\n",
        "\n",
        "---\n",
        "\n",
        "This notebook extracts metrics from the Fabric Capacity Metrics semantic model, including:\n",
        "\n",
        "- **Utilisation**: Interactive vs Background CU consumption\n",
        "- **Throttling**: Delay and rejection percentages, risk levels\n",
        "- **Carryforward/Overage**: Accumulated debt and recovery projections\n",
        "- **Health score**: Composite metric (0–100)\n",
        "- **SKU recommendation**: Based on usage and target utilisation\n",
        "\n",
        "### Prerequisites\n",
        "1. Install the Fabric Capacity Metrics app in your tenant\n",
        "2. Have capacity admin or app access permissions\n",
        "3. Move the Capacity Metrics workspace to a Fabric capacity (for XMLA access)\n",
        "4. Ensure XMLA endpoint is enabled on the capacity\n",
        "\n",
        "### Expected runtime\n",
        "**15–30+ minutes** – The notebook runs many DAX queries (one per day for timepoints and one per day for item/operation data). If you run via Fabric CLI, use a timeout of at least **1800 seconds (30 min)** or **3600 (1 hour)**.\n",
        "\n",
        "### How to Use\n",
        "1. **Run All** – Install/import runs first; if the kernel restarts after pip, run **Run All** again (configuration will re-run).\n",
        "2. Set **WORKSPACE_ID** and **DATASET_ID** in the Configuration section (from your Capacity Metrics app URL or semantic model Properties).\n",
        "3. Set DEBUG_MODE = True for detailed logging (recommended for first run).\n",
        "4. Review the report and SKU recommendation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Import Libraries & Optional Install\n",
        "\n",
        "**Import libraries first** (no pip, no kernel restart). Then the optional cell checks your Semantic Link version and installs/upgrades **semantic-link-labs** only. If that cell restarts the kernel, run **Run All** again – imports run first again, then configuration.\n",
        "\n",
        "Ref: [Keep Your Semantic Link Libraries Up to Date](https://www.data-nova.io/post/keep-your-semantic-link-libraries-up-to-date-in-fabric-notebooks) (Data Nova)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# Import libraries – run this first (no pip, no kernel restart)\n",
        "# =============================================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import sempy.fabric as fabric\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from plotly.io import to_html\n",
        "\n",
        "print(\"[SUCCESS] All libraries loaded (SemPy, pandas, plotly)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# Semantic Link version check + semantic-link-labs install (optional)\n",
        "# =============================================================================\n",
        "# We do NOT upgrade semantic-link (pre-installed). We only install/upgrade\n",
        "# semantic-link-labs. If the kernel restarts after this, run Run All again.\n",
        "#\n",
        "import subprocess\n",
        "import sys\n",
        "import json\n",
        "import urllib.request\n",
        "\n",
        "def get_latest_version(package_name):\n",
        "    \"\"\"Fetch latest version from PyPI\"\"\"\n",
        "    try:\n",
        "        url = f\"https://pypi.org/pypi/{package_name}/json\"\n",
        "        with urllib.request.urlopen(url, timeout=5) as response:\n",
        "            data = json.loads(response.read())\n",
        "            return data['info']['version']\n",
        "    except Exception as e:\n",
        "        print(f\"[WARNING] Could not fetch latest version: {e}\")\n",
        "        return None\n",
        "\n",
        "# Check semantic-link version (pre-installed – we do not reinstall)\n",
        "try:\n",
        "    from importlib.metadata import version, PackageNotFoundError\n",
        "    installed_version = version('semantic-link-sempy')\n",
        "    print(f\"[INFO] semantic-link (pre-installed): {installed_version}\")\n",
        "    latest_version = get_latest_version('semantic-link')\n",
        "    if latest_version:\n",
        "        try:\n",
        "            from packaging.version import Version\n",
        "            if Version(installed_version) < Version(latest_version):\n",
        "                print(f\"[INFO] Newer on PyPI: {latest_version} (Fabric manages this; no action needed)\")\n",
        "        except ImportError:\n",
        "            pass\n",
        "except PackageNotFoundError:\n",
        "    print(\"[INFO] semantic-link not found (unexpected in Fabric)\")\n",
        "\n",
        "# Install/upgrade semantic-link-labs only\n",
        "print(\"\\n[INFO] Installing/upgrading semantic-link-labs...\")\n",
        "result = subprocess.run(\n",
        "    [sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"--quiet\", \"semantic-link-labs\"],\n",
        "    capture_output=True,\n",
        "    text=True\n",
        ")\n",
        "if result.returncode == 0:\n",
        "    print(\"[SUCCESS] semantic-link-labs installed/upgraded\")\n",
        "else:\n",
        "    print(f\"[WARNING] pip message (often safe to ignore): {result.stderr[:300] if result.stderr else 'none'}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Configuration\n",
        "\n",
        "Set your Capacity Metrics workspace/dataset IDs and options here. If the kernel restarted after the install cell, run **Run All** again – config will re-run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# CONFIGURATION - Update these values for your environment\n",
        "# =============================================================================\n",
        "\n",
        "# ── Capacity Metrics Connection ──\n",
        "# Workspace, semantic model, and capacity IDs (replace with placeholders when replicating to public).\n",
        "WORKSPACE_ID = \"your-workspace-guid\"   # Workspace containing Capacity Metrics\n",
        "DATASET_ID   = \"your-dataset-guid\"   # Capacity Metrics semantic model ID\n",
        "\n",
        "# ── Analysis Mode ──\n",
        "# \"single\" = analyse one capacity (CAPACITY_ID required)\n",
        "# \"multi\"  = auto-discover and analyse all active capacities\n",
        "ANALYSIS_MODE = \"single\"      # \"single\" or \"multi\"\n",
        "CAPACITY_ID = \"your-capacity-guid\"   # Capacity to analyse (single mode)\n",
        "CAPACITY_IDS = None           # Optional for multi mode: list of GUIDs to filter, or None for all active\n",
        "\n",
        "# ── Analysis Settings ──\n",
        "DAYS_TO_ANALYZE = 14          # Number of days (max 14 for Capacity Metrics retention)\n",
        "NEEDS_FREE_VIEWERS = False    # Set True if you need F64+ for free viewers\n",
        "\n",
        "# ── Advanced Analysis ──\n",
        "WEEKDAY_WEEKEND_SPLIT = True  # Separate weekday vs weekend analysis for more accurate sizing\n",
        "TREND_ANALYSIS = True         # Show consumption trend and growth projection\n",
        "RESERVED_VS_PAYG = True       # Include reserved vs PAYG cost comparison\n",
        "SPIKE_FILTERING = True        # Filter settlement/pause catch-up spikes from P80 calculations\n",
        "\n",
        "# ── Output ──\n",
        "DEBUG_MODE = True             # True for detailed logging, False for clean output\n",
        "SAVE_TO_LAKEHOUSE = True      # Save results to Lakehouse Delta tables\n",
        "LAKEHOUSE_NAME = \"LH_Capacity_Advisor\"\n",
        "SAVE_HTML_REPORT = True       # Generate one HTML report per capacity\n",
        "REPLACE_EXISTING_OUTPUTS = True  # True: deterministic names (replace). False: keep history with timestamp suffix.\n",
        "\n",
        "# ── Validation ──\n",
        "if not WORKSPACE_ID or not WORKSPACE_ID.strip():\n",
        "    raise ValueError(\"WORKSPACE_ID is empty. Provide the workspace GUID containing Capacity Metrics.\")\n",
        "if not DATASET_ID or not DATASET_ID.strip():\n",
        "    raise ValueError(\"DATASET_ID is empty. Provide the Capacity Metrics semantic model GUID.\")\n",
        "\n",
        "WORKSPACE_ID = WORKSPACE_ID.strip()\n",
        "DATASET_ID = DATASET_ID.strip()\n",
        "\n",
        "_mode = ANALYSIS_MODE.strip().lower()\n",
        "if _mode not in (\"single\", \"multi\"):\n",
        "    raise ValueError(f\"ANALYSIS_MODE must be 'single' or 'multi', got: '{ANALYSIS_MODE}'\")\n",
        "\n",
        "if _mode == \"single\" and (not CAPACITY_ID or not CAPACITY_ID.strip()):\n",
        "    raise ValueError(\"ANALYSIS_MODE is 'single' but CAPACITY_ID is empty. Please provide a capacity GUID.\")\n",
        "\n",
        "if _mode == \"single\":\n",
        "    CAPACITY_ID = CAPACITY_ID.strip()\n",
        "    print(f\"[INFO] Mode: SINGLE capacity\")\n",
        "    print(f\"[INFO] Capacity ID: {CAPACITY_ID}\")\n",
        "else:\n",
        "    print(f\"[INFO] Mode: MULTI capacity (auto-discover all active)\")\n",
        "    if CAPACITY_IDS:\n",
        "        print(f\"[INFO] Filtered to {len(CAPACITY_IDS)} specific capacities\")\n",
        "\n",
        "print(f\"[INFO] Workspace: {WORKSPACE_ID}\")\n",
        "print(f\"[INFO] Dataset:   {DATASET_ID}\")\n",
        "print(f\"[INFO] Days: {DAYS_TO_ANALYZE} | Debug: {'ON' if DEBUG_MODE else 'OFF'}\")\n",
        "print(f\"[INFO] Lakehouse file policy: {'REPLACE existing files' if REPLACE_EXISTING_OUTPUTS else 'KEEP history (timestamped suffixes)'}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Reference Data & Helper Functions\n",
        "\n",
        "SKU definitions and thresholds used for recommendations.\n",
        "\n",
        "### Fabric SKU Tiers\n",
        "Each SKU has a fixed CU (Capacity Unit) rate per second. In a 30-second evaluation window:\n",
        "- F2 = 2 CUs/sec = 60 CUs per 30-sec window\n",
        "- F64 = 64 CUs/sec = 1,920 CUs per 30-sec window (minimum for free viewers)\n",
        "\n",
        "### Throttling Thresholds\n",
        "Microsoft applies throttling when capacity is overloaded:\n",
        "- **Interactive Delay**: Starts after 10 minutes of overload\n",
        "- **Interactive Rejection**: Starts after 60 minutes of overload\n",
        "- **Background Rejection**: Starts after 24 hours of overload"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# SKU Definitions - Microsoft Fabric capacity tiers\n",
        "# =============================================================================\n",
        "# CU rates and budgets from Microsoft documentation.\n",
        "# Pricing: published PAYG list prices (USD) and estimated 1-year reserved prices.\n",
        "# Actual costs vary by region, currency, and agreement type.\n",
        "# Check https://azure.microsoft.com/pricing/details/microsoft-fabric/\n",
        "# Prices last updated: 2025. Verify current pricing before making decisions.\n",
        "#\n",
        "SKUS = [\n",
        "    {\"name\": \"F2\",    \"cus_per_second\": 2,    \"budget_30s\": 60,    \"monthly_usd\": 262,    \"monthly_reserved_usd\": 155},\n",
        "    {\"name\": \"F4\",    \"cus_per_second\": 4,    \"budget_30s\": 120,   \"monthly_usd\": 525,    \"monthly_reserved_usd\": 310},\n",
        "    {\"name\": \"F8\",    \"cus_per_second\": 8,    \"budget_30s\": 240,   \"monthly_usd\": 1049,   \"monthly_reserved_usd\": 619},\n",
        "    {\"name\": \"F16\",   \"cus_per_second\": 16,   \"budget_30s\": 480,   \"monthly_usd\": 2099,   \"monthly_reserved_usd\": 1239},\n",
        "    {\"name\": \"F32\",   \"cus_per_second\": 32,   \"budget_30s\": 960,   \"monthly_usd\": 4198,   \"monthly_reserved_usd\": 2477},\n",
        "    {\"name\": \"F64\",   \"cus_per_second\": 64,   \"budget_30s\": 1920,  \"monthly_usd\": 8395,   \"monthly_reserved_usd\": 4954},   # Min for free viewers\n",
        "    {\"name\": \"F128\",  \"cus_per_second\": 128,  \"budget_30s\": 3840,  \"monthly_usd\": 16790,  \"monthly_reserved_usd\": 9907},\n",
        "    {\"name\": \"F256\",  \"cus_per_second\": 256,  \"budget_30s\": 7680,  \"monthly_usd\": 33580,  \"monthly_reserved_usd\": 19813},\n",
        "    {\"name\": \"F512\",  \"cus_per_second\": 512,  \"budget_30s\": 15360, \"monthly_usd\": 67161,  \"monthly_reserved_usd\": 39625},\n",
        "    {\"name\": \"F1024\", \"cus_per_second\": 1024, \"budget_30s\": 30720, \"monthly_usd\": 134321, \"monthly_reserved_usd\": 79250},\n",
        "    {\"name\": \"F2048\", \"cus_per_second\": 2048, \"budget_30s\": 61440, \"monthly_usd\": 268643, \"monthly_reserved_usd\": 158499},\n",
        "]\n",
        "\n",
        "# Target utilisation: 80% leaves headroom for peaks\n",
        "TARGET_UTILISATION = 0.80\n",
        "\n",
        "# Reserved pricing discount (approx 41% savings vs PAYG)\n",
        "RESERVED_DISCOUNT_PCT = 41\n",
        "RESERVED_BREAKEVEN_UTIL = 0.60  # Below this, PAYG + pause/resume may be cheaper\n",
        "\n",
        "print(f\"[INFO] {len(SKUS)} SKU tiers loaded (F2 to F2048)\")\n",
        "print(\"[INFO] Pricing: published PAYG and 1-year reserved list prices (USD). Your actual costs may differ.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# Helper Functions\n",
        "# =============================================================================\n",
        "import numpy as np\n",
        "\n",
        "def calculate_utilisation(daily_cus: float, sku: dict) -> float:\n",
        "    \"\"\"\n",
        "    Calculate utilisation ratio for a given daily CU consumption on a SKU.\n",
        "    Returns a decimal (0.75 = 75%).\n",
        "    \"\"\"\n",
        "    cus_per_window = daily_cus / 2880  # 2880 = 30-second windows per 24 hours\n",
        "    return cus_per_window / sku[\"budget_30s\"]\n",
        "\n",
        "\n",
        "def calculate_required_budget(daily_cus: float, target_utilisation: float = 0.80) -> float:\n",
        "    \"\"\"\n",
        "    Calculate the required 30-second CU budget to handle a daily load\n",
        "    at the target utilisation level.\n",
        "    \"\"\"\n",
        "    cus_per_window = daily_cus / 2880\n",
        "    return cus_per_window / target_utilisation\n",
        "\n",
        "\n",
        "def get_sku_status(avg_util: float, max_util: float, needs_free_viewers: bool, sku: dict) -> str:\n",
        "    \"\"\"\n",
        "    Classify a SKU's fit based on utilisation.\n",
        "    avg_util and max_util are decimals (0.75 = 75%).\n",
        "    \"\"\"\n",
        "    if needs_free_viewers and sku[\"cus_per_second\"] < 64:\n",
        "        return \"NO FREE VIEWERS\"\n",
        "    if max_util > 1.0:\n",
        "        return \"THROTTLING RISK\"\n",
        "    if avg_util > 0.95:\n",
        "        return \"TOO SMALL\"\n",
        "    if avg_util > 0.85:\n",
        "        return \"TIGHT\"\n",
        "    if avg_util > 0.60:\n",
        "        return \"GOOD FIT\"\n",
        "    if avg_util > 0.40:\n",
        "        return \"COMFORTABLE\"\n",
        "    return \"OVERSIZED\"\n",
        "\n",
        "\n",
        "def calculate_health_score(avg_util_pct: float, throttle_pct: float, carryover_pct: float) -> tuple:\n",
        "    \"\"\"\n",
        "    Composite capacity health score (0-100).\n",
        "\n",
        "    Weights: utilisation 40%, throttling 40%, carryforward 20%.\n",
        "    Returns (score, rating).\n",
        "    \"\"\"\n",
        "    # Utilisation component (40%)\n",
        "    if avg_util_pct <= 70:\n",
        "        util_score = 100\n",
        "    elif avg_util_pct <= 85:\n",
        "        util_score = 100 - ((avg_util_pct - 70) * 2)\n",
        "    elif avg_util_pct <= 100:\n",
        "        util_score = 70 - ((avg_util_pct - 85) * 3)\n",
        "    else:\n",
        "        util_score = max(0, 25 - ((avg_util_pct - 100) * 5))\n",
        "\n",
        "    # Throttling component (40%)\n",
        "    if throttle_pct == 0:\n",
        "        throttle_score = 100\n",
        "    elif throttle_pct < 5:\n",
        "        throttle_score = 80\n",
        "    elif throttle_pct < 15:\n",
        "        throttle_score = 50\n",
        "    elif throttle_pct < 30:\n",
        "        throttle_score = 25\n",
        "    else:\n",
        "        throttle_score = 0\n",
        "\n",
        "    # Carryforward component (20%)\n",
        "    if carryover_pct == 0:\n",
        "        carryover_score = 100\n",
        "    elif carryover_pct < 10:\n",
        "        carryover_score = 80\n",
        "    elif carryover_pct < 30:\n",
        "        carryover_score = 50\n",
        "    elif carryover_pct < 50:\n",
        "        carryover_score = 25\n",
        "    else:\n",
        "        carryover_score = 0\n",
        "\n",
        "    score = (util_score * 0.4) + (throttle_score * 0.4) + (carryover_score * 0.2)\n",
        "\n",
        "    if score >= 90:\n",
        "        rating = \"EXCELLENT\"\n",
        "    elif score >= 75:\n",
        "        rating = \"GOOD\"\n",
        "    elif score >= 50:\n",
        "        rating = \"FAIR\"\n",
        "    elif score >= 25:\n",
        "        rating = \"POOR\"\n",
        "    else:\n",
        "        rating = \"CRITICAL\"\n",
        "\n",
        "    return round(score, 1), rating\n",
        "\n",
        "\n",
        "def format_duration(minutes: float) -> str:\n",
        "    \"\"\"Format minutes into a readable duration.\"\"\"\n",
        "    if minutes < 60:\n",
        "        return f\"{minutes:.0f} min\"\n",
        "    elif minutes < 1440:\n",
        "        return f\"{minutes / 60:.1f} hours\"\n",
        "    else:\n",
        "        return f\"{minutes / 1440:.1f} days\"\n",
        "\n",
        "\n",
        "print(\"[INFO] Core SKU helper functions loaded\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Trend Analysis & Spike Detection\n",
        "Linear regression on daily CU consumption to detect growth/decline, plus spike filtering for pause/resume catch-up days.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# Trend Analysis & Spike Detection\n",
        "# =============================================================================\n",
        "\n",
        "def calculate_trend(daily_summary, column='ActualCUs_sum'):\n",
        "    \"\"\"\n",
        "    Calculate linear trend and growth rate for a daily metric.\n",
        "    Returns dict with slope, weekly_growth_pct, forecast_weeks_to_exceed (if applicable).\n",
        "    \"\"\"\n",
        "    if len(daily_summary) < 3:\n",
        "        return {'has_trend': False}\n",
        "\n",
        "    y = daily_summary[column].values.astype(float)\n",
        "    x = np.arange(len(y), dtype=float)\n",
        "\n",
        "    # Linear regression\n",
        "    n = len(x)\n",
        "    sum_x = np.sum(x)\n",
        "    sum_y = np.sum(y)\n",
        "    sum_xy = np.sum(x * y)\n",
        "    sum_x2 = np.sum(x * x)\n",
        "\n",
        "    denom = (n * sum_x2 - sum_x * sum_x)\n",
        "    if denom == 0:\n",
        "        return {'has_trend': False}\n",
        "\n",
        "    slope = (n * sum_xy - sum_x * sum_y) / denom\n",
        "    intercept = (sum_y - slope * sum_x) / n\n",
        "\n",
        "    # R-squared\n",
        "    y_pred = slope * x + intercept\n",
        "    ss_res = np.sum((y - y_pred) ** 2)\n",
        "    ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
        "    r_squared = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0\n",
        "\n",
        "    # Weekly growth rate\n",
        "    avg_val = np.mean(y) if np.mean(y) != 0 else 1\n",
        "    daily_growth_pct = (slope / avg_val) * 100\n",
        "    weekly_growth_pct = daily_growth_pct * 7\n",
        "\n",
        "    # Direction\n",
        "    if weekly_growth_pct > 2:\n",
        "        direction = \"GROWING\"\n",
        "    elif weekly_growth_pct < -2:\n",
        "        direction = \"DECLINING\"\n",
        "    else:\n",
        "        direction = \"STABLE\"\n",
        "\n",
        "    return {\n",
        "        'has_trend': True,\n",
        "        'slope': slope,\n",
        "        'intercept': intercept,\n",
        "        'r_squared': round(r_squared, 3),\n",
        "        'daily_growth_pct': round(daily_growth_pct, 2),\n",
        "        'weekly_growth_pct': round(weekly_growth_pct, 2),\n",
        "        'direction': direction,\n",
        "    }\n",
        "\n",
        "\n",
        "def detect_pause_spikes(daily_summary, threshold_factor=2.0):\n",
        "    \"\"\"\n",
        "    Detect days that are likely settlement or pause/resume catch-up spikes.\n",
        "    A spike is a day where peak CUs exceed the median by threshold_factor times.\n",
        "    Returns a boolean mask of spike days.\n",
        "    \"\"\"\n",
        "    if len(daily_summary) < 3:\n",
        "        return pd.Series([False] * len(daily_summary), index=daily_summary.index)\n",
        "\n",
        "    median_peak = daily_summary['ActualCUs_max'].median()\n",
        "    if median_peak == 0:\n",
        "        return pd.Series([False] * len(daily_summary), index=daily_summary.index)\n",
        "\n",
        "    return daily_summary['ActualCUs_max'] > (median_peak * threshold_factor)\n",
        "\n",
        "print(\"[INFO] Trend analysis & spike detection loaded\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Workspace Breakdown\n",
        "Groups item-level CU data by workspace, with optional billable vs non-billable split.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# Workspace Breakdown\n",
        "# =============================================================================\n",
        "\n",
        "def get_workspace_breakdown(df_items_cap):\n",
        "    \"\"\"\n",
        "    Extract workspace-level CU breakdown from item data.\n",
        "    Prefers Workspace Name over Workspace Id for display.\n",
        "    Returns a DataFrame grouped by workspace.\n",
        "    \"\"\"\n",
        "    if len(df_items_cap) == 0:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Prefer Workspace Name, fall back to Workspace Id\n",
        "    ws_name_cols = [c for c in df_items_cap.columns if 'workspace' in c.lower() and 'name' in c.lower()]\n",
        "    ws_id_cols = [c for c in df_items_cap.columns if 'workspace' in c.lower() and 'id' in c.lower()]\n",
        "\n",
        "    # Use name column only if it exists AND has non-empty values\n",
        "    ws_col = None\n",
        "    if ws_name_cols:\n",
        "        _name_col = ws_name_cols[0]\n",
        "        _valid = df_items_cap[_name_col].dropna().astype(str).str.strip().replace('', pd.NA).dropna()\n",
        "        if len(_valid) > 0:\n",
        "            ws_col = _name_col\n",
        "    if ws_col is None:\n",
        "        ws_col = ws_id_cols[0] if ws_id_cols else None\n",
        "\n",
        "    cu_cols = [c for c in df_items_cap.columns if 'CU' in c.upper() and 'Total' in c]\n",
        "    if not ws_col or not cu_cols:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    cu_col = cu_cols[0]\n",
        "\n",
        "    # Check for billing type column\n",
        "    billing_cols = [c for c in df_items_cap.columns if 'billing' in c.lower() and 'type' in c.lower()]\n",
        "\n",
        "    ws_summary = df_items_cap.groupby(ws_col).agg(\n",
        "        TotalCUs=(cu_col, 'sum'),\n",
        "        Operations=(cu_col, 'count'),\n",
        "    ).reset_index()\n",
        "    ws_summary.columns = ['Workspace', 'Total CUs', 'Operations']\n",
        "\n",
        "    # Add billable vs non-billable if available\n",
        "    if billing_cols:\n",
        "        bill_col = billing_cols[0]\n",
        "        billable = df_items_cap[df_items_cap[bill_col].astype(str).str.lower().str.contains('billable')]\n",
        "        nonbillable = df_items_cap[~df_items_cap[bill_col].astype(str).str.lower().str.contains('billable')]\n",
        "        bill_by_ws = billable.groupby(ws_col)[cu_col].sum().reset_index()\n",
        "        bill_by_ws.columns = ['Workspace', 'Billable CUs']\n",
        "        ws_summary = ws_summary.merge(bill_by_ws, on='Workspace', how='left')\n",
        "        ws_summary['Billable CUs'] = ws_summary['Billable CUs'].fillna(0)\n",
        "\n",
        "    ws_summary = ws_summary.sort_values('Total CUs', ascending=False)\n",
        "    total = ws_summary['Total CUs'].sum()\n",
        "    ws_summary['Share %'] = (ws_summary['Total CUs'] / total * 100).round(1) if total > 0 else 0\n",
        "    return ws_summary\n",
        "\n",
        "print(\"[INFO] Workspace breakdown loaded\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Connect & Detect Version\n",
        "\n",
        "The Capacity Metrics semantic model has multiple versions (v37, v40, v44, v47+) with different measure names. This section detects the version to choose the correct DAX queries.\n",
        "\n",
        "This approach ensures compatibility across different Capacity Metrics app versions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# Connect to Capacity Metrics & Detect Version\n",
        "# =============================================================================\n",
        "# The Capacity Metrics semantic model has multiple versions with different\n",
        "# measure names. We detect the version to use the correct DAX queries.\n",
        "#\n",
        "print(\"[INFO] Connecting to Capacity Metrics semantic model...\")\n",
        "\n",
        "start_time = time.time()\n",
        "version = None\n",
        "\n",
        "# Test for v53 (newer Capacity Metrics / FUAM; uses Capacity Id with capital C)\n",
        "try:\n",
        "    test_query_v53 = \"\"\"EVALUATE ROW(\"Test\", 'All Measures'[Blocked workspaces (Day)])\"\"\"\n",
        "    test_df = fabric.evaluate_dax(\n",
        "        workspace=WORKSPACE_ID, \n",
        "        dataset=DATASET_ID, \n",
        "        dax_string=test_query_v53\n",
        "    )\n",
        "    version = 'v53'\n",
        "    print(\"[SUCCESS] Detected Capacity Metrics version: v53 (latest)\")\n",
        "except Exception as e:\n",
        "    if DEBUG_MODE:\n",
        "        print(f\"[DEBUG] v53 test failed: {str(e)[:100]}\")\n",
        "\n",
        "# Test for v47 (CapacitiesList parameter; capacity Id lowercase)\n",
        "if version is None:\n",
        "    try:\n",
        "        test_query_v47 = \"\"\"DEFINE MPARAMETER 'DefaultCapacityID' = \"00000000-0000-0000-0000-000000000000\"\n",
        "        EVALUATE SUMMARIZECOLUMNS(\"Test\", [Background billable CU %])\"\"\"\n",
        "        test_df = fabric.evaluate_dax(\n",
        "            workspace=WORKSPACE_ID, \n",
        "            dataset=DATASET_ID, \n",
        "            dax_string=test_query_v47\n",
        "        )\n",
        "        version = 'v47'\n",
        "        print(\"[SUCCESS] Detected Capacity Metrics version: v47+\")\n",
        "    except Exception as e:\n",
        "        if DEBUG_MODE:\n",
        "            print(f\"[DEBUG] v47 test failed: {str(e)[:100]}\")\n",
        "\n",
        "# Test for v40-v44\n",
        "if version is None:\n",
        "    try:\n",
        "        test_query_v40 = \"\"\"EVALUATE ROW(\"Test\", 'All Measures'[Background billable CU %])\"\"\"\n",
        "        test_df = fabric.evaluate_dax(\n",
        "            workspace=WORKSPACE_ID, \n",
        "            dataset=DATASET_ID, \n",
        "            dax_string=test_query_v40\n",
        "        )\n",
        "        version = 'v40'\n",
        "        print(\"[SUCCESS] Detected Capacity Metrics version: v40-v44\")\n",
        "    except Exception as e:\n",
        "        if DEBUG_MODE:\n",
        "            print(f\"[DEBUG] v40 test failed: {str(e)[:100]}\")\n",
        "\n",
        "# Test for v37 (older version)\n",
        "if version is None:\n",
        "    try:\n",
        "        test_query_v37 = \"\"\"EVALUATE ROW(\"Test\", 'All Measures'[xBackground %])\"\"\"\n",
        "        test_df = fabric.evaluate_dax(\n",
        "            workspace=WORKSPACE_ID, \n",
        "            dataset=DATASET_ID, \n",
        "            dax_string=test_query_v37\n",
        "        )\n",
        "        version = 'v37'\n",
        "        print(\"[SUCCESS] Detected Capacity Metrics version: v37 (older)\")\n",
        "    except Exception as e:\n",
        "        if DEBUG_MODE:\n",
        "            print(f\"[DEBUG] v37 test failed: {str(e)[:100]}\")\n",
        "\n",
        "\n",
        "def _probe_column_exists(column_ref: str) -> tuple:\n",
        "    \"\"\"Return (exists, error_message). Uses a tiny DAX query to validate column metadata.\"\"\"\n",
        "    probe = f\"\"\"\n",
        "    EVALUATE\n",
        "    TOPN(1, SUMMARIZECOLUMNS({column_ref}))\n",
        "    \"\"\"\n",
        "    try:\n",
        "        fabric.evaluate_dax(\n",
        "            workspace=WORKSPACE_ID,\n",
        "            dataset=DATASET_ID,\n",
        "            dax_string=probe\n",
        "        )\n",
        "        return True, None\n",
        "    except Exception as ex:\n",
        "        return False, str(ex)\n",
        "\n",
        "\n",
        "def _validate_schema_for_version(ver: str) -> None:\n",
        "    \"\"\"Validate required/optional columns for detected Capacity Metrics version.\"\"\"\n",
        "\n",
        "    # Each required entry is: (label, [acceptable column references])\n",
        "    required_by_version = {\n",
        "        'v53': [\n",
        "            (\"Capacity id\", [\"Capacities[Capacity Id]\"]),\n",
        "            (\"Capacity name\", [\"Capacities[Capacity name]\"]),\n",
        "            (\"Capacity state\", [\"Capacities[State]\", \"Capacities[state]\"]),\n",
        "            (\"Timepoint date\", [\"TimePoints[Date]\"]),\n",
        "            (\"Timepoint key\", [\"TimePoints[TimePoint]\"]),\n",
        "            (\"Item operation date\", [\"'Metrics By Item Operation And Day'[Date]\"]),\n",
        "            (\"Item operation item id\", [\"'Metrics By Item Operation And Day'[Item Id]\"]),\n",
        "            (\"Item operation CU\", [\"'Metrics By Item Operation And Day'[CU (s)]\"]),\n",
        "            (\"Item name\", [\"'Items'[Item Name]\"]),\n",
        "            (\"Item kind\", [\"'Items'[Item Kind]\"]),\n",
        "            (\"Workspace name\", [\"'Items'[Workspace Name]\"]),\n",
        "        ],\n",
        "        'v47': [\n",
        "            (\"Capacity id\", [\"Capacities[capacity Id]\"]),\n",
        "            (\"Capacity name\", [\"Capacities[Capacity name]\"]),\n",
        "            (\"Capacity state\", [\"Capacities[State]\", \"Capacities[state]\"]),\n",
        "            (\"Timepoint date\", [\"TimePoints[Date]\"]),\n",
        "            (\"Timepoint key\", [\"TimePoints[TimePoint]\"]),\n",
        "            (\"Item operation date\", [\"'Metrics By Item Operation And Day'[Date]\"]),\n",
        "            (\"Item operation item id\", [\"'Metrics By Item Operation And Day'[Item Id]\"]),\n",
        "            (\"Item operation CU\", [\"'Metrics By Item Operation And Day'[CU (s)]\"]),\n",
        "            (\"Item name\", [\"'Items'[Item Name]\"]),\n",
        "            (\"Item kind\", [\"'Items'[Item Kind]\"]),\n",
        "            (\"Workspace name\", [\"'Items'[Workspace Name]\"]),\n",
        "        ],\n",
        "        'v40': [\n",
        "            (\"Capacity id\", [\"Capacities[capacity Id]\"]),\n",
        "            (\"Capacity name\", [\"Capacities[Capacity name]\"]),\n",
        "            (\"Capacity state\", [\"Capacities[State]\", \"Capacities[state]\"]),\n",
        "            (\"Timepoint date\", [\"TimePoints[Date]\"]),\n",
        "            (\"Timepoint key\", [\"TimePoints[TimePoint]\"]),\n",
        "            (\"Item operation date\", [\"'Metrics By Item Operation And Day'[Date]\"]),\n",
        "            (\"Item operation item id\", [\"'Metrics By Item Operation And Day'[Item Id]\"]),\n",
        "            (\"Item operation CU\", [\"'Metrics By Item Operation And Day'[CU (s)]\"]),\n",
        "            (\"Item name\", [\"'Items'[Item Name]\"]),\n",
        "            (\"Item kind\", [\"'Items'[Item Kind]\"]),\n",
        "            (\"Workspace name\", [\"'Items'[Workspace Name]\"]),\n",
        "        ],\n",
        "        'v37': [\n",
        "            (\"Capacity id\", [\"Capacities[capacityId]\"]),\n",
        "            (\"Capacity state\", [\"Capacities[state]\"]),\n",
        "            (\"Timepoint date\", [\"TimePoints[Date]\"]),\n",
        "            (\"Timepoint key\", [\"TimePoints[TimePoint]\"]),\n",
        "            (\"Item operation date\", [\"'MetricsByItemandOperationandDay'[Date]\"]),\n",
        "            (\"Item operation item id\", [\"'MetricsByItemandOperationandDay'[ItemId]\"]),\n",
        "            (\"Item operation CU\", [\"'MetricsByItemandOperationandDay'[sum_CU]\"]),\n",
        "            (\"Item name\", [\"'Items'[Item Name]\"]),\n",
        "            (\"Item kind\", [\"'Items'[ItemKind]\"]),\n",
        "            (\"Workspace name\", [\"'Items'[Workspace Name]\"]),\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    optional_by_version = {\n",
        "        'v53': [\n",
        "            (\"Billing type\", \"'Metrics By Item Operation And Day'[Billing type]\"),\n",
        "            (\"Overloaded minutes\", \"'Metrics By Item Operation And Day'[Overloaded minutes]\"),\n",
        "            (\"Throttling minutes\", \"'Metrics By Item Operation And Day'[Throttling (min)]\"),\n",
        "        ],\n",
        "        'v47': [\n",
        "            (\"Billing type\", \"'Metrics By Item Operation And Day'[Billing type]\"),\n",
        "            (\"Overloaded minutes\", \"'Metrics By Item Operation And Day'[Overloaded minutes]\"),\n",
        "            (\"Throttling minutes\", \"'Metrics By Item Operation And Day'[Throttling (min)]\"),\n",
        "        ],\n",
        "        'v40': [\n",
        "            (\"Billing type\", \"'Metrics By Item Operation And Day'[Billing type]\"),\n",
        "            (\"Overloaded minutes\", \"'Metrics By Item Operation And Day'[Overloaded minutes]\"),\n",
        "            (\"Throttling minutes\", \"'Metrics By Item Operation And Day'[Throttling (min)]\"),\n",
        "        ],\n",
        "        'v37': [\n",
        "            (\"Throttling minutes\", \"'MetricsByItemandOperationandDay'[Throttling (min)]\"),\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    required_specs = required_by_version.get(ver, [])\n",
        "    optional_specs = optional_by_version.get(ver, [])\n",
        "\n",
        "    missing_required = []\n",
        "    missing_optional = []\n",
        "\n",
        "    for label, alternatives in required_specs:\n",
        "        matched = False\n",
        "        errors = []\n",
        "        for col in alternatives:\n",
        "            ok, err = _probe_column_exists(col)\n",
        "            if ok:\n",
        "                matched = True\n",
        "                break\n",
        "            if err:\n",
        "                errors.append(err)\n",
        "\n",
        "        if not matched:\n",
        "            missing_required.append((label, alternatives, errors[0] if errors else \"Unknown error\"))\n",
        "\n",
        "    for label, col in optional_specs:\n",
        "        ok, err = _probe_column_exists(col)\n",
        "        if not ok:\n",
        "            missing_optional.append((label, col, err if err else \"Unknown error\"))\n",
        "\n",
        "    if missing_optional:\n",
        "        print(\"[WARNING] Optional columns not found (continuing):\")\n",
        "        for label, col, _ in missing_optional:\n",
        "            print(f\"  - {label}: {col}\")\n",
        "\n",
        "    if missing_required:\n",
        "        msg_lines = [\n",
        "            f\"Schema validation failed for version {ver}.\",\n",
        "            \"Missing REQUIRED columns:\",\n",
        "        ]\n",
        "        for label, alternatives, err in missing_required:\n",
        "            msg_lines.append(f\"- {label}: expected one of {alternatives}\")\n",
        "            if DEBUG_MODE:\n",
        "                msg_lines.append(f\"  First error: {err[:240]}\")\n",
        "\n",
        "        raise Exception(\"\\n\".join(msg_lines))\n",
        "\n",
        "    print(f\"[SUCCESS] Schema validation passed for version {ver}\")\n",
        "\n",
        "\n",
        "# Validate connection and schema\n",
        "elapsed = time.time() - start_time\n",
        "if version is None:\n",
        "    print(\"\\n[ERROR] Could not connect to Capacity Metrics semantic model\")\n",
        "    print(\"\\nTroubleshooting:\")\n",
        "    print(\"1. Verify XMLA endpoint is enabled (Capacity Settings > Power BI Workloads)\")\n",
        "    print(\"2. Ensure you have Build/Read permissions on the semantic model\")\n",
        "    print(\"3. Confirm the workspace is on a Fabric or Premium capacity\")\n",
        "    print(\"4. Check WORKSPACE_ID and DATASET_ID in the Configuration cell\")\n",
        "    raise Exception(\"Connection failed - see troubleshooting tips above\")\n",
        "else:\n",
        "    print(f\"[INFO] Connection established in {elapsed:.1f}s\")\n",
        "    _validate_schema_for_version(version)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. Extract Capacities\n",
        "\n",
        "Capacities are read from the Capacity Metrics app. Only **Active** capacities are included (Suspended are ignored). Set CAPACITY_ID for single capacity, or use multi to analyse all active."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# Extract Available Capacities\n",
        "# =============================================================================\n",
        "print(\"[INFO] Extracting capacity list...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Capacities table schema sourced from FUAM (Fabric Unified Admin Monitoring)\n",
        "# https://github.com/microsoft/fabric-toolbox/tree/main/monitoring/fabric-unified-admin-monitoring\n",
        "# Columns: Capacity Id, Capacity name, SKU, State, Owners, Region, Source\n",
        "capacity_query = \"\"\"EVALUATE SELECTCOLUMNS(\n",
        "    Capacities,\n",
        "    \"CapacityId\", Capacities[Capacity Id],\n",
        "    \"CapacityName\", Capacities[Capacity name],\n",
        "    \"SKU\", Capacities[SKU],\n",
        "    \"State\", Capacities[State]\n",
        ")\"\"\"\n",
        "\n",
        "try:\n",
        "    df_capacities = fabric.evaluate_dax(\n",
        "        workspace=WORKSPACE_ID,\n",
        "        dataset=DATASET_ID,\n",
        "        dax_string=capacity_query\n",
        "    )\n",
        "    elapsed = time.time() - start_time\n",
        "\n",
        "    # Standardise column names\n",
        "    df_capacities.columns = ['CapacityId', 'CapacityName', 'SKU', 'State']\n",
        "\n",
        "    # Clean up names: fall back to first 8 chars of ID if name is empty\n",
        "    df_capacities['CapacityName'] = df_capacities['CapacityName'].fillna('').astype(str).str.strip()\n",
        "    df_capacities.loc[df_capacities['CapacityName'] == '', 'CapacityName'] = df_capacities['CapacityId'].str[:8]\n",
        "\n",
        "    # Only Active capacities\n",
        "    df_active = df_capacities[df_capacities['State'].str.strip().str.lower() == 'active'].copy()\n",
        "    n_suspended = len(df_capacities) - len(df_active)\n",
        "    if n_suspended > 0:\n",
        "        print(f\"[INFO] Ignoring {n_suspended} suspended capacit{'y' if n_suspended == 1 else 'ies'}\")\n",
        "\n",
        "    if len(df_active) == 0:\n",
        "        raise ValueError(\"No Active capacities found in Capacity Metrics.\")\n",
        "\n",
        "    print(f\"[SUCCESS] Found {len(df_active)} active capacit{'y' if len(df_active) == 1 else 'ies'} ({elapsed:.1f}s)\")\n",
        "    for idx, row in df_active.iterrows():\n",
        "        print(f\"  \\u2713 {row['CapacityName']} ({row['SKU']}) - {row['CapacityId'][:8]}...\")\n",
        "\n",
        "    # Build lookups\n",
        "    CAPACITY_NAMES = dict(zip(df_active['CapacityId'], df_active['CapacityName']))\n",
        "    CAPACITY_SKUS  = dict(zip(df_active['CapacityId'], df_active['SKU']))\n",
        "\n",
        "    # -- Build CAPACITY_ID_LIST based on ANALYSIS_MODE --\n",
        "    _mode = ANALYSIS_MODE.strip().lower()\n",
        "\n",
        "    if _mode == \"single\":\n",
        "        if CAPACITY_ID not in df_active['CapacityId'].values:\n",
        "            available = [(r['CapacityName'], r['SKU'], r['CapacityId']) for _, r in df_active.iterrows()]\n",
        "            raise ValueError(\n",
        "                f\"CAPACITY_ID '{CAPACITY_ID}' is not found or not Active.\\n\"\n",
        "                f\"Available active capacities: {available}\"\n",
        "            )\n",
        "        CAPACITY_ID_LIST = [CAPACITY_ID]\n",
        "        print(f\"[INFO] Single mode: {CAPACITY_NAMES[CAPACITY_ID]} ({CAPACITY_SKUS[CAPACITY_ID]})\")\n",
        "\n",
        "    else:  # multi\n",
        "        if CAPACITY_IDS:\n",
        "            df_filtered = df_active[df_active['CapacityId'].isin(CAPACITY_IDS)].copy()\n",
        "            missing = set(CAPACITY_IDS) - set(df_filtered['CapacityId'].tolist())\n",
        "            if missing:\n",
        "                print(f\"[WARNING] Not found or not Active: {missing}\")\n",
        "            if len(df_filtered) == 0:\n",
        "                raise ValueError(\"None of the specified CAPACITY_IDS are Active.\")\n",
        "            CAPACITY_ID_LIST = df_filtered['CapacityId'].tolist()\n",
        "        else:\n",
        "            CAPACITY_ID_LIST = df_active['CapacityId'].tolist()\n",
        "        print(f\"[INFO] Multi mode: analysing {len(CAPACITY_ID_LIST)} capacities\")\n",
        "\n",
        "    print(f\"[INFO] Capacities to analyse:\")\n",
        "    for cid in CAPACITY_ID_LIST:\n",
        "        print(f\"  - {CAPACITY_NAMES.get(cid, cid[:8])} ({CAPACITY_SKUS.get(cid, '?')})\")\n",
        "\n",
        "    if DEBUG_MODE:\n",
        "        display(df_active[['CapacityName', 'SKU', 'CapacityId', 'State']])\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"[ERROR] {e}\")\n",
        "    raise\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6. Extract Timepoint Data (30-second granularity)\n",
        "\n",
        "Timepoint data (30-second granularity) is used for consumption, throttling, and carryforward, including:\n",
        "- **Utilisation %**: How much of your capacity is being used\n",
        "- **Throttling %**: Operations being delayed or rejected\n",
        "- **Carryforward %**: Accumulated \"debt\" from overages\n",
        "\n",
        "This data is extracted day-by-day due to the volume of records (2,880 timepoints per day)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# Extract Timepoint-Level Metrics\n",
        "# =============================================================================\n",
        "# This is the core data extraction - 30-second granularity metrics including\n",
        "# utilisation, throttling percentages, and carryforward data.\n",
        "#\n",
        "# We iterate day-by-day to manage data volume and avoid timeouts.\n",
        "#\n",
        "print(\"[INFO] Extracting timepoint-level metrics...\")\n",
        "print(f\"[INFO] Analyzing {DAYS_TO_ANALYZE} days of data\")\n",
        "\n",
        "# Calculate date range\n",
        "end_date = datetime.now()\n",
        "start_date = end_date - timedelta(days=DAYS_TO_ANALYZE)\n",
        "\n",
        "all_timepoints = []\n",
        "extraction_start = time.time()\n",
        "\n",
        "# Build the DAX query template based on version\n",
        "def build_timepoint_query(capacity_id: str, year: int, month: int, day: int, ver: str) -> str:\n",
        "    \"\"\"\n",
        "    Build DAX query for timepoint data based on semantic model version.\n",
        "    \n",
        "    This query extracts metrics for each 30-second timepoint:\n",
        "    - Background/Interactive billable CU %\n",
        "    - Throttling metrics (delay %, rejection %)\n",
        "    - Carryforward metrics (add %, burndown %, cumulative %)\n",
        "    - Recovery estimates (expected burndown in minutes)\n",
        "    \"\"\"\n",
        "    \n",
        "    # v53 uses Capacity Id (capital C); v47 uses capacity Id (lowercase c)\n",
        "    cap_col = \"Capacities[Capacity Id]\" if ver == 'v53' else \"Capacities[capacity Id]\"\n",
        "    if ver in ['v53', 'v47']:\n",
        "        return f\"\"\"\n",
        "        DEFINE\n",
        "            MPARAMETER 'CapacitiesList' = {{ \"{capacity_id}\" }}\n",
        "        \n",
        "        VAR __Data = \n",
        "            SUMMARIZECOLUMNS(\n",
        "                {cap_col},\n",
        "                'TimePoints'[TimePoint],\n",
        "                FILTER(Capacities, {cap_col} = \"{capacity_id}\"),\n",
        "                FILTER(TimePoints, 'TimePoints'[Date] = DATE({year}, {month}, {day})),\n",
        "                \"BackgroundPct\", 'All Measures'[Background billable CU %],\n",
        "                \"InteractivePct\", 'All Measures'[Interactive billable CU %],\n",
        "                \"SKUPct\", 'All Measures'[SKU CU by timepoint %],\n",
        "                \"CULimit\", 'All Measures'[CU limit],\n",
        "                \"SKUCUByTimepoint\", 'All Measures'[SKU CU by timepoint],\n",
        "                \"InteractiveDelayPct\", 'All Measures'[Dynamic interactive delay %],\n",
        "                \"InteractiveRejectPct\", 'All Measures'[Dynamic interactive rejection %],\n",
        "                \"InteractiveRejectThreshold\", 'All Measures'[Interactive rejection threshold],\n",
        "                \"BackgroundRejectPct\", 'All Measures'[Dynamic background rejection %],\n",
        "                \"BackgroundRejectThreshold\", 'All Measures'[Background rejection threshold],\n",
        "                \"CarryoverAddPct\", 'All Measures'[Carry over add %],\n",
        "                \"CarryoverBurndownPct\", 'All Measures'[Carry over burndown %],\n",
        "                \"CarryoverCumulativePct\", 'All Measures'[Cumulative carry over %],\n",
        "                \"OverageRefLine\", 'All Measures'[Overage reference line],\n",
        "                \"ExpectedBurndownMin\", 'All Measures'[Expected burndown in minutes]\n",
        "            )\n",
        "        EVALUATE __Data\n",
        "        \"\"\"\n",
        "    \n",
        "    elif ver == 'v40':\n",
        "        return f\"\"\"\n",
        "        DEFINE\n",
        "            MPARAMETER 'CapacityID' = \"{capacity_id}\"\n",
        "        \n",
        "        VAR __Data = \n",
        "            SUMMARIZECOLUMNS(\n",
        "                Capacities[capacity Id],\n",
        "                'TimePoints'[TimePoint],\n",
        "                FILTER(Capacities, Capacities[capacity Id] = \"{capacity_id}\"),\n",
        "                FILTER(TimePoints, 'TimePoints'[Date] = DATE({year}, {month}, {day})),\n",
        "                \"BackgroundPct\", 'All Measures'[Background billable CU %],\n",
        "                \"InteractivePct\", 'All Measures'[Interactive billable CU %],\n",
        "                \"SKUPct\", 'All Measures'[SKU CU by timepoint %],\n",
        "                \"CULimit\", 'All Measures'[CU limit],\n",
        "                \"SKUCUByTimepoint\", 'All Measures'[SKU CU by timepoint],\n",
        "                \"InteractiveDelayPct\", 'All Measures'[Dynamic interactive delay %],\n",
        "                \"InteractiveRejectPct\", 'All Measures'[Dynamic interactive rejection %],\n",
        "                \"InteractiveRejectThreshold\", 'All Measures'[Interactive rejection threshold],\n",
        "                \"BackgroundRejectPct\", 'All Measures'[Dynamic background rejection %],\n",
        "                \"BackgroundRejectThreshold\", 'All Measures'[Background rejection threshold],\n",
        "                \"CarryoverAddPct\", 'All Measures'[Carry over add %],\n",
        "                \"CarryoverBurndownPct\", 'All Measures'[Carry over burndown %],\n",
        "                \"CarryoverCumulativePct\", 'All Measures'[Cumulative carry over %],\n",
        "                \"OverageRefLine\", 'All Measures'[Overage reference line],\n",
        "                \"ExpectedBurndownMin\", 'All Measures'[Expected burndown in minutes]\n",
        "            )\n",
        "        EVALUATE __Data\n",
        "        \"\"\"\n",
        "    \n",
        "    else:  # v37\n",
        "        return f\"\"\"\n",
        "        DEFINE\n",
        "            MPARAMETER 'CapacityID' = \"{capacity_id}\"\n",
        "        \n",
        "        VAR __Data = \n",
        "            SUMMARIZECOLUMNS(\n",
        "                Capacities[capacityId],\n",
        "                'TimePoints'[TimePoint],\n",
        "                FILTER(Capacities, Capacities[capacityId] = \"{capacity_id}\"),\n",
        "                FILTER(TimePoints, 'TimePoints'[Date] = DATE({year}, {month}, {day})),\n",
        "                \"BackgroundPct\", 'All Measures'[xBackground %],\n",
        "                \"InteractivePct\", 'All Measures'[xInteractive %],\n",
        "                \"SKUPct\", 'All Measures'[SKU CU by TimePoint %],\n",
        "                \"CULimit\", 'All Measures'[CU Limit],\n",
        "                \"SKUCUByTimepoint\", 'All Measures'[SKU CU by TimePoint],\n",
        "                \"InteractiveDelayPct\", 'All Measures'[Dynamic InteractiveDelay %],\n",
        "                \"InteractiveRejectPct\", 'All Measures'[Dynamic InteractiveRejection %],\n",
        "                \"InteractiveRejectThreshold\", 'All Measures'[Interactive rejection threshold],\n",
        "                \"BackgroundRejectPct\", 'All Measures'[Dynamic BackgroundRejection %],\n",
        "                \"BackgroundRejectThreshold\", 'All Measures'[Background rejection threshold],\n",
        "                \"CarryoverAddPct\", 'All Measures'[xCarryOver_added %],\n",
        "                \"CarryoverBurndownPct\", 'All Measures'[xCarryOver_burndown %],\n",
        "                \"CarryoverCumulativePct\", 'All Measures'[xCarryOver_Cumulative %],\n",
        "                \"OverageRefLine\", 'All Measures'[Overage reference line],\n",
        "                \"ExpectedBurndownMin\", 'All Measures'[Expected burndown in minutes]\n",
        "            )\n",
        "        EVALUATE __Data\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "# Iterate through each capacity and day\n",
        "for cap_idx, cap_row in df_capacities.iterrows():\n",
        "    capacity_id = cap_row['CapacityId']\n",
        "    print(f\"\\n[INFO] Processing capacity: {capacity_id}\")\n",
        "    \n",
        "    # Generate date list\n",
        "    current_date = start_date\n",
        "    while current_date <= end_date:\n",
        "        year = current_date.year\n",
        "        month = current_date.month\n",
        "        day = current_date.day\n",
        "        date_label = current_date.strftime('%Y-%m-%d')\n",
        "        \n",
        "        try:\n",
        "            query = build_timepoint_query(capacity_id, year, month, day, version)\n",
        "            df_day = fabric.evaluate_dax(\n",
        "                workspace=WORKSPACE_ID,\n",
        "                dataset=DATASET_ID,\n",
        "                dax_string=query\n",
        "            )\n",
        "            \n",
        "            if len(df_day) > 0:\n",
        "                all_timepoints.append(df_day)\n",
        "                if DEBUG_MODE:\n",
        "                    print(f\"  {date_label}: {len(df_day)} timepoints\")\n",
        "            else:\n",
        "                    pass\n",
        "        except Exception as e:\n",
        "            print(f\"  [WARNING] {date_label}: Query failed - {str(e)[:50]}\")\n",
        "        \n",
        "        current_date += timedelta(days=1)\n",
        "\n",
        "# Combine all data\n",
        "if all_timepoints:\n",
        "    df_timepoints = pd.concat(all_timepoints, ignore_index=True)\n",
        "    elapsed = time.time() - extraction_start\n",
        "    print(f\"\\n[SUCCESS] Extracted {len(df_timepoints):,} timepoint records ({elapsed:.1f}s)\")\n",
        "    \n",
        "    if DEBUG_MODE:\n",
        "        display(df_timepoints.head(3))\n",
        "else:\n",
        "    print(\"\\n[ERROR] No timepoint data extracted\")\n",
        "    df_timepoints = pd.DataFrame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 7. Extract Item/Operation Data\n",
        "\n",
        "Item-level data shows which workloads (reports, dataflows, notebooks, etc.) are consuming capacity:\n",
        "- Top consumers that might need optimization\n",
        "- Failed/rejected operations indicating problems\n",
        "- Throttling at the item level\n",
        "\n",
        "**No item-level rows?** Ensure WORKSPACE_ID and DATASET_ID point to the **Capacity Metrics app** (not FUAM's semantic model). See **Context/ITEM-LEVEL-DATA-AND-FUAM.md** for troubleshooting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# Extract Item/Operation Level Metrics\n",
        "# =============================================================================\n",
        "# Extracts consumption data per item per operation per day,\n",
        "# including workspace name, item name, billing type, and throttling minutes.\n",
        "#\n",
        "import re\n",
        "print(\"[INFO] Extracting item/operation level metrics...\")\n",
        "\n",
        "# Optional measures for v40/v47: (output_name, column_name_in_model, dax_expr).\n",
        "# If a column is missing (MS schema change), we omit it (use 0) and retry.\n",
        "ITEM_OPTIONAL_MEASURES_V47 = [\n",
        "    (\"ThrottlingMin\", \"Throttling (min)\", \"SUM('{table_name}'[Throttling (min)])\"),\n",
        "    (\"OverloadedMin\", \"Overloaded minutes\", \"SUM('{table_name}'[Overloaded minutes])\"),\n",
        "    (\"UserCount\", \"Users\", \"SUM('{table_name}'[Users])\"),\n",
        "    (\"SuccessCount\", \"Successful operations\", \"SUM('{table_name}'[Successful operations])\"),\n",
        "    (\"RejectCount\", \"Rejected operations\", \"SUM('{table_name}'[Rejected operations])\"),\n",
        "    (\"TotalOps\", \"Operations\", \"SUM('{table_name}'[Operations])\"),\n",
        "    (\"InvalidCount\", \"Invalid operations\", \"SUM('{table_name}'[Invalid operations])\"),\n",
        "    (\"FailedCount\", \"Failed operations\", \"SUM('{table_name}'[Failed operations])\"),\n",
        "    (\"CancelledCount\", \"Cancelled operations\", \"SUM('{table_name}'[Cancelled operations])\"),\n",
        "]\n",
        "\n",
        "def _parse_missing_column_error(exc):\n",
        "    \"\"\"Extract column name from 'Column ... cannot be found' DAX error.\"\"\"\n",
        "    msg = str(exc)\n",
        "    if \"cannot be found\" not in msg and \"may not be used\" not in msg:\n",
        "        return None\n",
        "    m = re.search(r\"Column\\s+['\\\"]?(?:<[^>]+>)?([^'\\\"<>]+)(?:</[^>]+>)?['\\\"]?\", msg)\n",
        "    return m.group(1).strip() if m else None\n",
        "\n",
        "def _build_item_measures_v47(skip_columns, table_name):\n",
        "    \"\"\"Build measure list for v47/v40 item query; use 0 for columns in skip_columns (schema drift).\"\"\"\n",
        "    lines = [\n",
        "        (\"ThrottlingMin\", \"Throttling (min)\", f\"SUM('{table_name}'[Throttling (min)])\"),\n",
        "        (\"OverloadedMin\", \"Overloaded minutes\", f\"SUM('{table_name}'[Overloaded minutes])\"),\n",
        "        (\"UserCount\", \"Users\", f\"SUM('{table_name}'[Users])\"),\n",
        "        (\"SuccessCount\", \"Successful operations\", f\"SUM('{table_name}'[Successful operations])\"),\n",
        "        (\"RejectCount\", \"Rejected operations\", f\"SUM('{table_name}'[Rejected operations])\"),\n",
        "        (\"TotalOps\", \"Operations\", f\"SUM('{table_name}'[Operations])\"),\n",
        "        (\"InvalidCount\", \"Invalid operations\", f\"SUM('{table_name}'[Invalid operations])\"),\n",
        "        (\"FailedCount\", \"Failed operations\", f\"SUM('{table_name}'[Failed operations])\"),\n",
        "        (\"CancelledCount\", \"Cancelled operations\", f\"SUM('{table_name}'[Cancelled operations])\"),\n",
        "    ]\n",
        "    parts = [f'\"{out_name}\", {expr}' if col_name not in skip_columns else f'\"{out_name}\", 0' for out_name, col_name, expr in lines]\n",
        "    return \",\\n                \".join(parts)\n",
        "\n",
        "def build_item_query(capacity_id: str, year: int, month: int, day: int, ver: str, skip_columns=None) -> str:\n",
        "    \"\"\"Build DAX query for item-level metrics. skip_columns = set of column names to omit (use 0) when model schema differs.\"\"\"\n",
        "    skip_columns = skip_columns or set()\n",
        "\n",
        "    if ver in ['v53', 'v47', 'v40']:\n",
        "        param_name = \"CapacitiesList\" if ver in ['v53', 'v47'] else \"CapacityID\"\n",
        "        param_format = f'{{ \"{capacity_id}\" }}' if ver in ['v53', 'v47'] else f'\"{capacity_id}\"'\n",
        "        # v53 uses Capacity Id (capital C); v47/v40 use capacity Id (lowercase c)\n",
        "        cap_col = \"Capacities[Capacity Id]\" if ver == 'v53' else \"Capacities[capacity Id]\"\n",
        "        table_name = \"Metrics By Item Operation And Day\"\n",
        "        measures_str = _build_item_measures_v47(skip_columns, table_name)\n",
        "\n",
        "        return f\"\"\"\n",
        "        DEFINE\n",
        "            MPARAMETER '{param_name}' = {param_format}\n",
        "\n",
        "        VAR __Filter =\n",
        "            FILTER(\n",
        "                KEEPFILTERS(VALUES('{table_name}'[Date])),\n",
        "                '{table_name}'[Date] = DATE({year}, {month}, {day})\n",
        "            )\n",
        "\n",
        "        VAR __Data =\n",
        "            SUMMARIZECOLUMNS(\n",
        "                {cap_col},\n",
        "                Items[Workspace Id],\n",
        "                'Items'[Workspace Name],\n",
        "                '{table_name}'[Date],\n",
        "                '{table_name}'[Item Id],\n",
        "                'Items'[Item Name],\n",
        "                'Items'[Item Kind],\n",
        "                '{table_name}'[Operation name],\n",
        "                '{table_name}'[Billing type],\n",
        "                FILTER(Capacities, {cap_col} = \"{capacity_id}\"),\n",
        "                __Filter,\n",
        "                \"DurationSec\", SUM('{table_name}'[Duration (s)]),\n",
        "                \"TotalCUs\", SUM('{table_name}'[CU (s)]),\n",
        "                {measures_str}\n",
        "            )\n",
        "\n",
        "        EVALUATE\n",
        "            FILTER(__Data, [TotalCUs] > 0)\n",
        "        ORDER BY [TotalCUs] DESC\n",
        "        \"\"\"\n",
        "\n",
        "    else:  # v37\n",
        "        return f\"\"\"\n",
        "        DEFINE\n",
        "            MPARAMETER 'CapacityID' = \"{capacity_id}\"\n",
        "\n",
        "        VAR __Filter =\n",
        "            FILTER(\n",
        "                KEEPFILTERS(VALUES('MetricsByItemandOperationandDay'[Date])),\n",
        "                'MetricsByItemandOperationandDay'[Date] = DATE({year}, {month}, {day})\n",
        "            )\n",
        "\n",
        "        VAR __Data =\n",
        "            SUMMARIZECOLUMNS(\n",
        "                Capacities[capacityId],\n",
        "                Items[WorkspaceId],\n",
        "                'Items'[Workspace Name],\n",
        "                'MetricsByItemandOperationandDay'[Date],\n",
        "                'MetricsByItemandOperationandDay'[ItemId],\n",
        "                'Items'[Item Name],\n",
        "                'Items'[ItemKind],\n",
        "                'MetricsByItemandOperationandDay'[OperationName],\n",
        "                FILTER(Capacities, Capacities[capacityId] = \"{capacity_id}\"),\n",
        "                __Filter,\n",
        "                \"DurationSec\", SUM('MetricsByItemandOperationandDay'[sum_duration]),\n",
        "                \"TotalCUs\", SUM('MetricsByItemandOperationandDay'[sum_CU]),\n",
        "                \"ThrottlingMin\", SUM('MetricsByItemandOperationandDay'[Throttling (min)]),\n",
        "                \"UserCount\", SUM('MetricsByItemandOperationandDay'[count_users]),\n",
        "                \"SuccessCount\", SUM('MetricsByItemandOperationandDay'[count_successful_operations]),\n",
        "                \"RejectCount\", SUM('MetricsByItemandOperationandDay'[count_rejected_operations]),\n",
        "                \"TotalOps\", SUM('MetricsByItemandOperationandDay'[count_operations]),\n",
        "                \"InvalidCount\", SUM('MetricsByItemandOperationandDay'[count_Invalid_operations]),\n",
        "                \"FailedCount\", SUM('MetricsByItemandOperationandDay'[count_failure_operations]),\n",
        "                \"CancelledCount\", SUM('MetricsByItemandOperationandDay'[count_cancelled_operations])\n",
        "            )\n",
        "\n",
        "        EVALUATE\n",
        "            FILTER(__Data, [TotalCUs] > 0)\n",
        "        ORDER BY [TotalCUs] DESC\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "def build_item_query_range(capacity_id: str, start_dt, end_dt, ver: str, skip_columns=None, include_billing=True, cap_col_override=None) -> str:\n",
        "    \"\"\"Build a range-based item query (more robust than day-by-day for schema drift/data sparsity).\"\"\"\n",
        "    skip_columns = skip_columns or set()\n",
        "\n",
        "    sy, sm, sd = start_dt.year, start_dt.month, start_dt.day\n",
        "    ey, em, ed = end_dt.year, end_dt.month, end_dt.day\n",
        "\n",
        "    if ver in ['v53', 'v47', 'v40']:\n",
        "        param_name = \"CapacitiesList\" if ver in ['v53', 'v47'] else \"CapacityID\"\n",
        "        param_format = f'{{ \"{capacity_id}\" }}' if ver in ['v53', 'v47'] else f'\"{capacity_id}\"'\n",
        "        default_cap_col = \"Capacities[Capacity Id]\" if ver == 'v53' else \"Capacities[capacity Id]\"\n",
        "        cap_col = cap_col_override or default_cap_col\n",
        "        table_name = \"Metrics By Item Operation And Day\"\n",
        "        measures_str = _build_item_measures_v47(skip_columns, table_name)\n",
        "        billing_line = f\"'{table_name}'[Billing type],\" if include_billing else \"\"\n",
        "\n",
        "        return f\"\"\"\n",
        "        DEFINE\n",
        "            MPARAMETER '{param_name}' = {param_format}\n",
        "\n",
        "        VAR __Filter =\n",
        "            FILTER(\n",
        "                KEEPFILTERS(VALUES('{table_name}'[Date])),\n",
        "                '{table_name}'[Date] >= DATE({sy}, {sm}, {sd})\n",
        "                    && '{table_name}'[Date] <= DATE({ey}, {em}, {ed})\n",
        "            )\n",
        "\n",
        "        VAR __Data =\n",
        "            SUMMARIZECOLUMNS(\n",
        "                {cap_col},\n",
        "                Items[Workspace Id],\n",
        "                'Items'[Workspace Name],\n",
        "                '{table_name}'[Date],\n",
        "                '{table_name}'[Item Id],\n",
        "                'Items'[Item Name],\n",
        "                'Items'[Item Kind],\n",
        "                '{table_name}'[Operation name],\n",
        "                {billing_line}\n",
        "                FILTER(Capacities, {cap_col} = \"{capacity_id}\"),\n",
        "                __Filter,\n",
        "                \"DurationSec\", SUM('{table_name}'[Duration (s)]),\n",
        "                \"TotalCUs\", SUM('{table_name}'[CU (s)]),\n",
        "                {measures_str}\n",
        "            )\n",
        "\n",
        "        EVALUATE\n",
        "            FILTER(__Data, [TotalCUs] > 0)\n",
        "        ORDER BY [TotalCUs] DESC\n",
        "        \"\"\"\n",
        "\n",
        "    # v37\n",
        "    return f\"\"\"\n",
        "    DEFINE\n",
        "        MPARAMETER 'CapacityID' = \"{capacity_id}\"\n",
        "\n",
        "    VAR __Filter =\n",
        "        FILTER(\n",
        "            KEEPFILTERS(VALUES('MetricsByItemandOperationandDay'[Date])),\n",
        "            'MetricsByItemandOperationandDay'[Date] >= DATE({sy}, {sm}, {sd})\n",
        "                && 'MetricsByItemandOperationandDay'[Date] <= DATE({ey}, {em}, {ed})\n",
        "        )\n",
        "\n",
        "    VAR __Data =\n",
        "        SUMMARIZECOLUMNS(\n",
        "            Capacities[capacityId],\n",
        "            Items[WorkspaceId],\n",
        "            'Items'[Workspace Name],\n",
        "            'MetricsByItemandOperationandDay'[Date],\n",
        "            'MetricsByItemandOperationandDay'[ItemId],\n",
        "            'Items'[Item Name],\n",
        "            'Items'[ItemKind],\n",
        "            'MetricsByItemandOperationandDay'[OperationName],\n",
        "            FILTER(Capacities, Capacities[capacityId] = \"{capacity_id}\"),\n",
        "            __Filter,\n",
        "            \"DurationSec\", SUM('MetricsByItemandOperationandDay'[sum_duration]),\n",
        "            \"TotalCUs\", SUM('MetricsByItemandOperationandDay'[sum_CU]),\n",
        "            \"ThrottlingMin\", SUM('MetricsByItemandOperationandDay'[Throttling (min)]),\n",
        "            \"UserCount\", SUM('MetricsByItemandOperationandDay'[count_users]),\n",
        "            \"SuccessCount\", SUM('MetricsByItemandOperationandDay'[count_successful_operations]),\n",
        "            \"RejectCount\", SUM('MetricsByItemandOperationandDay'[count_rejected_operations]),\n",
        "            \"TotalOps\", SUM('MetricsByItemandOperationandDay'[count_operations]),\n",
        "            \"InvalidCount\", SUM('MetricsByItemandOperationandDay'[count_Invalid_operations]),\n",
        "            \"FailedCount\", SUM('MetricsByItemandOperationandDay'[count_failure_operations]),\n",
        "            \"CancelledCount\", SUM('MetricsByItemandOperationandDay'[count_cancelled_operations])\n",
        "        )\n",
        "\n",
        "    EVALUATE\n",
        "        FILTER(__Data, [TotalCUs] > 0)\n",
        "    ORDER BY [TotalCUs] DESC\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "all_items = []\n",
        "item_start = time.time()\n",
        "item_error_logged = False\n",
        "item_skip_columns = set()\n",
        "optional_item_columns = {c for _, c, _ in ITEM_OPTIONAL_MEASURES_V47}\n",
        "\n",
        "for _, cap_row in df_capacities.iterrows():\n",
        "    capacity_id = cap_row['CapacityId']\n",
        "    include_billing = True\n",
        "    extracted_for_capacity = False\n",
        "\n",
        "    if version in ['v53', 'v47', 'v40']:\n",
        "        cap_col_candidates = [\n",
        "            \"Capacities[Capacity Id]\",\n",
        "            \"Capacities[capacity Id]\",\n",
        "        ]\n",
        "        if version in ['v47', 'v40']:\n",
        "            cap_col_candidates = [\"Capacities[capacity Id]\", \"Capacities[Capacity Id]\"]\n",
        "\n",
        "        for cap_col_try in cap_col_candidates:\n",
        "            done = False\n",
        "            max_retries = 15\n",
        "            while not done and max_retries > 0:\n",
        "                try:\n",
        "                    query = build_item_query_range(\n",
        "                        capacity_id=capacity_id,\n",
        "                        start_dt=start_date,\n",
        "                        end_dt=end_date,\n",
        "                        ver=version,\n",
        "                        skip_columns=item_skip_columns,\n",
        "                        include_billing=include_billing,\n",
        "                        cap_col_override=cap_col_try,\n",
        "                    )\n",
        "                    df_items_cap = fabric.evaluate_dax(\n",
        "                        workspace=WORKSPACE_ID,\n",
        "                        dataset=DATASET_ID,\n",
        "                        dax_string=query\n",
        "                    )\n",
        "\n",
        "                    if len(df_items_cap) > 0:\n",
        "                        all_items.append(df_items_cap)\n",
        "                        extracted_for_capacity = True\n",
        "                        print(f\"  [INFO] Item rows for {capacity_id[:8]} using {cap_col_try}: {len(df_items_cap):,}\")\n",
        "                    done = True\n",
        "\n",
        "                except Exception as e:\n",
        "                    missing_col = _parse_missing_column_error(e)\n",
        "\n",
        "                    if missing_col == \"Billing type\":\n",
        "                        include_billing = False\n",
        "                        print(\"  [INFO] Omitting column not in model (schema drift): 'Billing type'\")\n",
        "                        max_retries -= 1\n",
        "                        continue\n",
        "\n",
        "                    if missing_col in optional_item_columns:\n",
        "                        if missing_col not in item_skip_columns:\n",
        "                            print(f\"  [INFO] Omitting column not in model (schema drift): '{missing_col}'\")\n",
        "                        item_skip_columns.add(missing_col)\n",
        "                        max_retries -= 1\n",
        "                        continue\n",
        "\n",
        "                    if not item_error_logged:\n",
        "                        print(f\"  [WARNING] Item query failed (first occurrence): {e}\")\n",
        "                        item_error_logged = True\n",
        "                    if DEBUG_MODE:\n",
        "                        print(f\"  [DEBUG] Item query failed for capacity {capacity_id} using {cap_col_try}\")\n",
        "                    done = True\n",
        "                    max_retries = 0\n",
        "\n",
        "            if extracted_for_capacity:\n",
        "                break\n",
        "\n",
        "    else:\n",
        "        try:\n",
        "            query = build_item_query_range(\n",
        "                capacity_id=capacity_id,\n",
        "                start_dt=start_date,\n",
        "                end_dt=end_date,\n",
        "                ver=version,\n",
        "                skip_columns=item_skip_columns,\n",
        "                include_billing=False,\n",
        "                cap_col_override=None,\n",
        "            )\n",
        "            df_items_cap = fabric.evaluate_dax(\n",
        "                workspace=WORKSPACE_ID,\n",
        "                dataset=DATASET_ID,\n",
        "                dax_string=query\n",
        "            )\n",
        "            if len(df_items_cap) > 0:\n",
        "                all_items.append(df_items_cap)\n",
        "                extracted_for_capacity = True\n",
        "                print(f\"  [INFO] Item rows for {capacity_id[:8]}: {len(df_items_cap):,}\")\n",
        "        except Exception as e:\n",
        "            if not item_error_logged:\n",
        "                print(f\"  [WARNING] Item query failed (first occurrence): {e}\")\n",
        "                item_error_logged = True\n",
        "\n",
        "    if not extracted_for_capacity:\n",
        "        print(f\"  [WARNING] No item rows for capacity {capacity_id[:8]} in selected date range\")\n",
        "\n",
        "# Combine item data\n",
        "if all_items:\n",
        "    df_items = pd.concat(all_items, ignore_index=True)\n",
        "    elapsed = time.time() - item_start\n",
        "    print(f\"[SUCCESS] Extracted {len(df_items):,} item/operation records ({elapsed:.1f}s)\")\n",
        "\n",
        "    # Standardize column names\n",
        "    col_map = {col: col.replace('[', '').replace(']', '').strip() for col in df_items.columns}\n",
        "    df_items = df_items.rename(columns=col_map)\n",
        "\n",
        "    if DEBUG_MODE:\n",
        "        # Show column names for verification\n",
        "        print(f\"[DEBUG] Columns: {list(df_items.columns)}\")\n",
        "        display(df_items.head(3))\n",
        "else:\n",
        "    print(\"[WARNING] No item-level data extracted\")\n",
        "    df_items = pd.DataFrame()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 8. Process & Analyze Data\n",
        "\n",
        "This section aggregates the extracted data and calculates:\n",
        "- Daily aggregates for trending\n",
        "- Throttling summaries\n",
        "- Carryforward analysis\n",
        "- Item-level rankings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Capacity Analysis Pipeline\n",
        "\n",
        "This section defines reusable functions and runs the full analysis pipeline **once per active capacity**.\n",
        "\n",
        "In **single** mode it processes one capacity with full inline output. In **multi** mode it loops through all active capacities with compact progress output.\n",
        "\n",
        "Each capacity gets:\n",
        "\n",
        "- Timepoint processing and daily summary\n",
        "- Key metric calculations and health score\n",
        "- SKU recommendation (weekday-filtered, spike-aware)\n",
        "- Charts: health gauge, utilisation, throttling, carryforward, SKU comparison, top workloads\n",
        "- Individual HTML report saved to Lakehouse (if enabled)\n",
        "\n",
        "Results are collected in `all_capacity_results` for cross-capacity comparison.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# Analysis Functions: Processing, Metrics, SKU Recommendation\n",
        "# =============================================================================\n",
        "\n",
        "def process_timepoints_for_capacity(df_timepoints_all, capacity_id):\n",
        "    \"\"\"\n",
        "    Filter and process timepoint data for a single capacity.\n",
        "    Returns (daily_summary DataFrame, current_sku dict or None).\n",
        "    \"\"\"\n",
        "    if len(df_timepoints_all) == 0:\n",
        "        return pd.DataFrame(), None\n",
        "\n",
        "    # Find capacity ID column\n",
        "    cap_col_candidates = [c for c in df_timepoints_all.columns if 'capacity' in c.lower() and 'id' in c.lower()]\n",
        "    cap_col = cap_col_candidates[0] if cap_col_candidates else df_timepoints_all.columns[0]\n",
        "    df_tp = df_timepoints_all[df_timepoints_all[cap_col].astype(str).str.strip() == capacity_id].copy()\n",
        "\n",
        "    if len(df_tp) == 0:\n",
        "        return pd.DataFrame(), None\n",
        "\n",
        "    # Standardize column names (DAX may return with brackets)\n",
        "    col_map = {col: col.replace('[', '').replace(']', '').strip() for col in df_tp.columns}\n",
        "    df_tp = df_tp.rename(columns=col_map)\n",
        "\n",
        "    # Find TimePoint column\n",
        "    tp_candidates = [c for c in df_tp.columns if 'TimePoint' in c or ('Time' in c and 'Point' in c)]\n",
        "    tp_col = tp_candidates[0] if tp_candidates else (df_tp.columns[1] if len(df_tp.columns) >= 2 else None)\n",
        "    if tp_col is None:\n",
        "        raise ValueError(f\"Could not find TimePoint column. Columns: {list(df_tp.columns)}\")\n",
        "\n",
        "    df_tp['TimePoint'] = pd.to_datetime(df_tp[tp_col])\n",
        "    df_tp['Date'] = df_tp['TimePoint'].dt.date\n",
        "\n",
        "    # Scale percentage columns from 0-1 to 0-100\n",
        "    pct_cols = [c for c in df_tp.columns if 'Pct' in c or '%' in c]\n",
        "    for col in pct_cols:\n",
        "        df_tp[col] = pd.to_numeric(df_tp[col], errors='coerce').fillna(0) * 100\n",
        "\n",
        "    # Detect current SKU from the CU budget per 30-sec window\n",
        "    current_sku = None\n",
        "    sku_budget_30s = None\n",
        "    if 'SKUCUByTimepoint' in df_tp.columns:\n",
        "        _sku_vals = pd.to_numeric(df_tp['SKUCUByTimepoint'], errors='coerce').dropna()\n",
        "        if len(_sku_vals) > 0:\n",
        "            sku_budget_30s = _sku_vals.mode().iloc[0]\n",
        "            current_sku = next((s for s in SKUS if s['budget_30s'] == sku_budget_30s), None)\n",
        "\n",
        "    # Calculate actual CUs per timepoint from utilisation % and SKU budget\n",
        "    # This is more reliable than the cumulative measure which is a running total\n",
        "    if sku_budget_30s and sku_budget_30s > 0:\n",
        "        df_tp['ActualCUs'] = (df_tp['BackgroundPct'] + df_tp['InteractivePct']) / 100.0 * sku_budget_30s\n",
        "    elif 'SKUCUByTimepoint' in df_tp.columns:\n",
        "        _sku_col = pd.to_numeric(df_tp['SKUCUByTimepoint'], errors='coerce').fillna(0)\n",
        "        df_tp['ActualCUs'] = (df_tp['BackgroundPct'] + df_tp['InteractivePct']) / 100.0 * _sku_col\n",
        "    else:\n",
        "        df_tp['ActualCUs'] = 0\n",
        "        print(\"  [WARNING] Cannot calculate CUs: no SKU budget data available\")\n",
        "\n",
        "    # Daily summaries\n",
        "    daily_summary = df_tp.groupby('Date').agg({\n",
        "        'BackgroundPct': ['mean', 'max'],\n",
        "        'InteractivePct': ['mean', 'max'],\n",
        "        'InteractiveDelayPct': ['mean', 'max'],\n",
        "        'InteractiveRejectPct': ['mean', 'max'],\n",
        "        'BackgroundRejectPct': ['mean', 'max'],\n",
        "        'CarryoverCumulativePct': ['mean', 'max'],\n",
        "        'ExpectedBurndownMin': ['mean', 'max'],\n",
        "        'ActualCUs': ['sum', 'max'],  # sum = daily total, max = peak window\n",
        "    }).reset_index()\n",
        "\n",
        "    daily_summary.columns = ['_'.join(col).strip('_') if col[1] else col[0]\n",
        "                             for col in daily_summary.columns.values]\n",
        "    daily_summary['TotalUtilPct_mean'] = daily_summary['BackgroundPct_mean'] + daily_summary['InteractivePct_mean']\n",
        "    daily_summary['TotalUtilPct_max'] = daily_summary['BackgroundPct_max'] + daily_summary['InteractivePct_max']\n",
        "\n",
        "    # Add day-of-week for weekday/weekend analysis\n",
        "    daily_summary['DayOfWeek'] = pd.to_datetime(daily_summary['Date']).dt.dayofweek  # 0=Mon, 6=Sun\n",
        "    daily_summary['IsWeekday'] = daily_summary['DayOfWeek'] < 5\n",
        "\n",
        "    # Detect pause/resume spikes\n",
        "    daily_summary['IsSuspectedSpike'] = detect_pause_spikes(daily_summary)\n",
        "\n",
        "    return daily_summary, current_sku\n",
        "\n",
        "\n",
        "def calculate_capacity_metrics(daily_summary):\n",
        "    \"\"\"Calculate all key metrics from daily summary. Returns a dict.\"\"\"\n",
        "    if len(daily_summary) == 0:\n",
        "        return None\n",
        "\n",
        "    m = {}\n",
        "    m['days_analyzed'] = len(daily_summary)\n",
        "\n",
        "    # -------------------------------------------------------------------\n",
        "    # Core metrics (all days)\n",
        "    # -------------------------------------------------------------------\n",
        "    m['avg_daily_cus'] = daily_summary['ActualCUs_sum'].mean()\n",
        "    m['max_daily_cus'] = daily_summary['ActualCUs_sum'].max()\n",
        "    m['p80_daily_cus'] = daily_summary['ActualCUs_sum'].quantile(0.8)\n",
        "\n",
        "    # Utilisation\n",
        "    m['avg_util'] = daily_summary['TotalUtilPct_mean'].mean()\n",
        "    m['max_util'] = daily_summary['TotalUtilPct_max'].max()\n",
        "    m['avg_interactive'] = daily_summary['InteractivePct_mean'].mean()\n",
        "    m['avg_background'] = daily_summary['BackgroundPct_mean'].mean()\n",
        "\n",
        "    # Throttling\n",
        "    m['avg_delay_pct'] = daily_summary['InteractiveDelayPct_mean'].mean()\n",
        "    m['max_delay_pct'] = daily_summary['InteractiveDelayPct_max'].max()\n",
        "    m['avg_int_reject_pct'] = daily_summary['InteractiveRejectPct_mean'].mean()\n",
        "    m['max_int_reject_pct'] = daily_summary['InteractiveRejectPct_max'].max()\n",
        "    m['avg_bg_reject_pct'] = daily_summary['BackgroundRejectPct_mean'].mean()\n",
        "    m['max_bg_reject_pct'] = daily_summary['BackgroundRejectPct_max'].max()\n",
        "\n",
        "    # Carryforward\n",
        "    m['avg_carryover_pct'] = daily_summary['CarryoverCumulativePct_mean'].mean()\n",
        "    m['max_carryover_pct'] = daily_summary['CarryoverCumulativePct_max'].max()\n",
        "    m['avg_burndown_min'] = daily_summary['ExpectedBurndownMin_mean'].mean()\n",
        "    m['max_burndown_min'] = daily_summary['ExpectedBurndownMin_max'].max()\n",
        "\n",
        "    # Days with issues\n",
        "    m['days_with_delay'] = int((daily_summary['InteractiveDelayPct_max'] > 0).sum())\n",
        "    m['days_with_rejection'] = int((daily_summary['InteractiveRejectPct_max'] > 0).sum())\n",
        "    m['days_with_carryover'] = int((daily_summary['CarryoverCumulativePct_max'] > 0).sum())\n",
        "\n",
        "    # Health score\n",
        "    m['health_score'], m['health_rating'] = calculate_health_score(\n",
        "        m['avg_util'],\n",
        "        max(m['avg_delay_pct'], m['avg_int_reject_pct']),\n",
        "        m['avg_carryover_pct']\n",
        "    )\n",
        "\n",
        "    # -------------------------------------------------------------------\n",
        "    # Weekday vs Weekend split\n",
        "    # -------------------------------------------------------------------\n",
        "    weekdays = daily_summary[daily_summary['IsWeekday']]\n",
        "    weekends = daily_summary[~daily_summary['IsWeekday']]\n",
        "\n",
        "    if len(weekdays) > 0:\n",
        "        m['weekday_avg_cus'] = weekdays['ActualCUs_sum'].mean()\n",
        "        m['weekday_p80_cus'] = weekdays['ActualCUs_sum'].quantile(0.8)\n",
        "        m['weekday_avg_util'] = weekdays['TotalUtilPct_mean'].mean()\n",
        "        m['weekday_count'] = len(weekdays)\n",
        "    else:\n",
        "        m['weekday_avg_cus'] = m['avg_daily_cus']\n",
        "        m['weekday_p80_cus'] = m['p80_daily_cus']\n",
        "        m['weekday_avg_util'] = m['avg_util']\n",
        "        m['weekday_count'] = 0\n",
        "\n",
        "    if len(weekends) > 0:\n",
        "        m['weekend_avg_cus'] = weekends['ActualCUs_sum'].mean()\n",
        "        m['weekend_p80_cus'] = weekends['ActualCUs_sum'].quantile(0.8)\n",
        "        m['weekend_avg_util'] = weekends['TotalUtilPct_mean'].mean()\n",
        "        m['weekend_count'] = len(weekends)\n",
        "    else:\n",
        "        m['weekend_avg_cus'] = 0\n",
        "        m['weekend_p80_cus'] = 0\n",
        "        m['weekend_avg_util'] = 0\n",
        "        m['weekend_count'] = 0\n",
        "\n",
        "    # Weekday/weekend ratio\n",
        "    if m['weekend_avg_cus'] > 0:\n",
        "        m['weekday_weekend_ratio'] = round(m['weekday_avg_cus'] / m['weekend_avg_cus'], 1)\n",
        "    else:\n",
        "        m['weekday_weekend_ratio'] = float('inf')\n",
        "\n",
        "    # -------------------------------------------------------------------\n",
        "    # Spike-filtered P80 (excludes suspected pause/resume spikes)\n",
        "    # -------------------------------------------------------------------\n",
        "    non_spike = daily_summary[~daily_summary['IsSuspectedSpike']]\n",
        "    m['spike_days_detected'] = int(daily_summary['IsSuspectedSpike'].sum())\n",
        "    if len(non_spike) >= 3:\n",
        "        m['p80_daily_cus_filtered'] = non_spike['ActualCUs_sum'].quantile(0.8)\n",
        "        m['weekday_p80_cus_filtered'] = non_spike[non_spike['IsWeekday']]['ActualCUs_sum'].quantile(0.8) if len(non_spike[non_spike['IsWeekday']]) > 0 else m['weekday_p80_cus']\n",
        "    else:\n",
        "        m['p80_daily_cus_filtered'] = m['p80_daily_cus']\n",
        "        m['weekday_p80_cus_filtered'] = m['weekday_p80_cus']\n",
        "\n",
        "    # -------------------------------------------------------------------\n",
        "    # Trend analysis\n",
        "    # -------------------------------------------------------------------\n",
        "    m['trend'] = calculate_trend(daily_summary, 'ActualCUs_sum')\n",
        "\n",
        "    return m\n",
        "\n",
        "\n",
        "def recommend_sku_for_capacity(metrics, skus, needs_free_viewers):\n",
        "    \"\"\"\n",
        "    Recommend SKU using the 80/80 approach:\n",
        "    find the smallest SKU where P80 daily consumption fits at 80% utilisation.\n",
        "\n",
        "    When weekday/weekend split is enabled and SPIKE_FILTERING is on,\n",
        "    uses the weekday P80 (filtered) for sizing, as weekdays typically drive\n",
        "    the highest sustained load.\n",
        "    \"\"\"\n",
        "    # Use weekday P80 (spike-filtered) if weekday split is enabled\n",
        "    if WEEKDAY_WEEKEND_SPLIT and SPIKE_FILTERING:\n",
        "        p80_cus = metrics.get('weekday_p80_cus_filtered', metrics['p80_daily_cus'])\n",
        "    elif WEEKDAY_WEEKEND_SPLIT:\n",
        "        p80_cus = metrics.get('weekday_p80_cus', metrics['p80_daily_cus'])\n",
        "    elif SPIKE_FILTERING:\n",
        "        p80_cus = metrics.get('p80_daily_cus_filtered', metrics['p80_daily_cus'])\n",
        "    else:\n",
        "        p80_cus = metrics['p80_daily_cus']\n",
        "\n",
        "    required_budget = calculate_required_budget(p80_cus, TARGET_UTILISATION)\n",
        "\n",
        "    sku_analysis = []\n",
        "    recommended_sku = None\n",
        "\n",
        "    for sku in skus:\n",
        "        avg_util_sku = calculate_utilisation(metrics['avg_daily_cus'], sku)\n",
        "        max_util_sku = calculate_utilisation(metrics['max_daily_cus'], sku)\n",
        "        p80_util_sku = calculate_utilisation(p80_cus, sku)\n",
        "        status = get_sku_status(avg_util_sku, max_util_sku, needs_free_viewers, sku)\n",
        "\n",
        "        # Reserved vs PAYG\n",
        "        payg = sku.get(\"monthly_usd\", 0)\n",
        "        reserved = sku.get(\"monthly_reserved_usd\", 0)\n",
        "        savings = payg - reserved\n",
        "\n",
        "        sku_analysis.append({\n",
        "            \"SKU\": sku[\"name\"],\n",
        "            \"CUs/sec\": sku[\"cus_per_second\"],\n",
        "            \"Daily Budget\": sku[\"budget_30s\"] * 2880,\n",
        "            \"Avg Util %\": round(avg_util_sku * 100, 1),\n",
        "            \"Peak Util %\": round(max_util_sku * 100, 1),\n",
        "            \"P80 Util %\": round(p80_util_sku * 100, 1),\n",
        "            \"Status\": status,\n",
        "            \"PAYG $/mo\": payg,\n",
        "            \"Reserved $/mo\": reserved,\n",
        "            \"Savings $/mo\": savings,\n",
        "        })\n",
        "\n",
        "        # Recommend: smallest SKU whose budget meets the P80-at-80% requirement\n",
        "        if recommended_sku is None and sku[\"budget_30s\"] >= required_budget:\n",
        "            if not (needs_free_viewers and sku[\"cus_per_second\"] < 64):\n",
        "                recommended_sku = sku\n",
        "\n",
        "    # Fallback: smallest that doesn't throttle on peak\n",
        "    if recommended_sku is None:\n",
        "        for sku in skus:\n",
        "            if calculate_utilisation(metrics['max_daily_cus'], sku) <= 1.0:\n",
        "                if not (needs_free_viewers and sku[\"cus_per_second\"] < 64):\n",
        "                    recommended_sku = sku\n",
        "                    break\n",
        "\n",
        "    if recommended_sku is None:\n",
        "        recommended_sku = skus[-1]\n",
        "\n",
        "    return sku_analysis, recommended_sku\n",
        "\n",
        "\n",
        "def filter_items_for_capacity(df_items_all, capacity_id):\n",
        "    \"\"\"Filter item-level data for a single capacity.\"\"\"\n",
        "    if len(df_items_all) == 0:\n",
        "        return pd.DataFrame()\n",
        "    cap_col_candidates = [c for c in df_items_all.columns if 'capacity' in c.lower() and 'id' in c.lower()]\n",
        "    if not cap_col_candidates:\n",
        "        return df_items_all\n",
        "    cap_col = cap_col_candidates[0]\n",
        "    return df_items_all[df_items_all[cap_col].astype(str).str.strip() == capacity_id].copy()\n",
        "\n",
        "\n",
        "print(\"[INFO] Analysis functions loaded\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# Analysis Functions: Chart Generation\n",
        "# =============================================================================\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "def create_capacity_charts(daily_summary, metrics, sku_analysis, recommended_sku, current_sku, df_items_cap):\n",
        "    \"\"\"Create all Plotly charts for a capacity. Returns dict of figure objects.\"\"\"\n",
        "    charts = {}\n",
        "\n",
        "    def _apply_chart_spacing(fig):\n",
        "        \"\"\"Apply consistent spacing so axis/label text remains visible across charts.\"\"\"\n",
        "        fig.update_layout(\n",
        "            margin=dict(l=90, r=90, t=90, b=90),\n",
        "            uniformtext_minsize=9,\n",
        "            uniformtext_mode=\"hide\",\n",
        "        )\n",
        "        fig.update_xaxes(automargin=True, title_standoff=16)\n",
        "        fig.update_yaxes(automargin=True, title_standoff=16)\n",
        "        fig.update_traces(cliponaxis=False, selector=dict(type=\"bar\"))\n",
        "        return fig\n",
        "\n",
        "    # 1. Health Gauge\n",
        "    fig_health = go.Figure(go.Indicator(\n",
        "        mode=\"gauge+number+delta\",\n",
        "        value=metrics['health_score'],\n",
        "        domain={'x': [0, 1], 'y': [0, 1]},\n",
        "        title={'text': f\"Capacity Health Score<br><span style='font-size:0.6em;color:gray'>{metrics['health_rating']}</span>\"},\n",
        "        gauge={\n",
        "            'axis': {'range': [0, 100], 'tickwidth': 1},\n",
        "            'bar': {'color': \"darkblue\"},\n",
        "            'steps': [\n",
        "                {'range': [0, 25], 'color': '#dc3545'},\n",
        "                {'range': [25, 50], 'color': '#fd7e14'},\n",
        "                {'range': [50, 75], 'color': '#ffc107'},\n",
        "                {'range': [75, 90], 'color': '#28a745'},\n",
        "                {'range': [90, 100], 'color': '#20c997'}\n",
        "            ],\n",
        "            'threshold': {'line': {'color': \"black\", 'width': 4}, 'thickness': 0.75, 'value': metrics['health_score']}\n",
        "        }\n",
        "    ))\n",
        "    fig_health.update_layout(height=300, margin=dict(l=20, r=20, t=60, b=20), font={'size': 14})\n",
        "    charts['fig_health'] = _apply_chart_spacing(fig_health)\n",
        "\n",
        "    # 2. Utilisation Gauges (Average & Peak on recommended SKU)\n",
        "    title_sku = recommended_sku[\"name\"]\n",
        "    avg_util_pct = calculate_utilisation(metrics['avg_daily_cus'], recommended_sku) * 100\n",
        "    peak_util_pct = calculate_utilisation(metrics['max_daily_cus'], recommended_sku) * 100\n",
        "\n",
        "    steps = [\n",
        "        {\"range\": [0, 40], \"color\": \"#EBF2F9\"},\n",
        "        {\"range\": [40, 80], \"color\": \"#D4EDDA\"},\n",
        "        {\"range\": [80, 95], \"color\": \"#FFF3CD\"},\n",
        "        {\"range\": [95, 100], \"color\": \"#F8D7DA\"},\n",
        "    ]\n",
        "    fig_util = make_subplots(rows=1, cols=2, specs=[[{\"type\": \"indicator\"}, {\"type\": \"indicator\"}]], horizontal_spacing=0.12)\n",
        "    fig_util.add_trace(go.Indicator(\n",
        "        mode=\"gauge+number\", value=round(avg_util_pct, 1),\n",
        "        title={\"text\": \"Average\", \"font\": {\"size\": 16, \"color\": \"#333\"}},\n",
        "        number={\"suffix\": \"%\", \"valueformat\": \".1f\", \"font\": {\"size\": 28, \"color\": \"#2D65BC\"}},\n",
        "        gauge={\"axis\": {\"range\": [0, 100], \"ticksuffix\": \"%\"}, \"bar\": {\"color\": \"#6A8DDC\"}, \"bgcolor\": \"white\", \"steps\": steps,\n",
        "               \"threshold\": {\"line\": {\"color\": \"#28A745\", \"width\": 3}, \"thickness\": 0.8, \"value\": 80}},\n",
        "    ), row=1, col=1)\n",
        "    fig_util.add_trace(go.Indicator(\n",
        "        mode=\"gauge+number\", value=round(peak_util_pct, 1),\n",
        "        title={\"text\": \"Peak\", \"font\": {\"size\": 16, \"color\": \"#333\"}},\n",
        "        number={\"suffix\": \"%\", \"valueformat\": \".1f\", \"font\": {\"size\": 28, \"color\": \"#2D65BC\"}},\n",
        "        gauge={\"axis\": {\"range\": [0, 100], \"ticksuffix\": \"%\"}, \"bar\": {\"color\": \"#F59E0B\"}, \"bgcolor\": \"white\", \"steps\": steps,\n",
        "               \"threshold\": {\"line\": {\"color\": \"#DC3545\", \"width\": 3}, \"thickness\": 0.8, \"value\": 95}},\n",
        "    ), row=1, col=2)\n",
        "    fig_util.update_layout(title=None, height=280, margin=dict(l=40, r=40, t=70, b=20), paper_bgcolor=\"white\",\n",
        "        annotations=[dict(text=f\"Utilisation on {title_sku}\", x=0.5, y=1.02, xref=\"paper\", yref=\"paper\", showarrow=False, font=dict(size=18), xanchor=\"center\")])\n",
        "    charts['fig_util_gauges'] = _apply_chart_spacing(fig_util)\n",
        "\n",
        "    # 3. Daily Utilisation (with weekday/weekend colouring and trend line)\n",
        "    if len(daily_summary) > 0:\n",
        "        fig_daily = go.Figure()\n",
        "        dates_str = pd.to_datetime(daily_summary['Date']).dt.strftime('%d %b')\n",
        "        day_names = pd.to_datetime(daily_summary['Date']).dt.strftime('%a')\n",
        "\n",
        "        # Colour weekends differently\n",
        "        bg_colors = ['rgba(40, 167, 69, 0.5)' if wd else 'rgba(40, 167, 69, 0.25)' for wd in daily_summary['IsWeekday']]\n",
        "        int_colors = ['rgba(102, 126, 234, 0.8)' if wd else 'rgba(102, 126, 234, 0.4)' for wd in daily_summary['IsWeekday']]\n",
        "\n",
        "        # Mark spike days\n",
        "        spike_mask = daily_summary['IsSuspectedSpike']\n",
        "\n",
        "        fig_daily.add_trace(go.Bar(\n",
        "            x=dates_str, y=daily_summary['BackgroundPct_mean'],\n",
        "            name='Background', marker_color=bg_colors,\n",
        "            text=[f\"{d}\" for d in day_names], textposition='none',\n",
        "            hovertemplate='%{x} (%{text})<br>Background: %{y:.1f}%<extra></extra>'\n",
        "        ))\n",
        "        fig_daily.add_trace(go.Bar(\n",
        "            x=dates_str, y=daily_summary['InteractivePct_mean'],\n",
        "            name='Interactive', marker_color=int_colors,\n",
        "            hovertemplate='%{x}<br>Interactive: %{y:.1f}%<extra></extra>'\n",
        "        ))\n",
        "\n",
        "        # Mark suspected spikes with red markers\n",
        "        if spike_mask.any():\n",
        "            spike_dates = dates_str[spike_mask]\n",
        "            spike_vals = daily_summary.loc[spike_mask, 'TotalUtilPct_mean']\n",
        "            fig_daily.add_trace(go.Scatter(\n",
        "                x=spike_dates, y=spike_vals + 3,\n",
        "                mode='markers+text', name='Suspected Spike',\n",
        "                marker=dict(symbol='triangle-down', size=12, color='red'),\n",
        "                text=['SPIKE'] * len(spike_dates), textposition='top center',\n",
        "                textfont=dict(size=9, color='red'),\n",
        "                hovertemplate='%{x}<br>Suspected pause/settlement spike<extra></extra>'\n",
        "            ))\n",
        "\n",
        "        # Trend line (fit on utilisation %, not raw CU scale)\n",
        "        if TREND_ANALYSIS:\n",
        "            t = calculate_trend(daily_summary, 'TotalUtilPct_mean')\n",
        "            if t.get('has_trend'):\n",
        "                x_vals = list(range(len(daily_summary)))\n",
        "                trend_y = [t['intercept'] + t['slope'] * xi for xi in x_vals]\n",
        "                fig_daily.add_trace(go.Scatter(\n",
        "                    x=dates_str, y=trend_y,\n",
        "                    mode='lines', name=f\"Trend ({t['direction']}, {t['weekly_growth_pct']:+.1f}%/wk)\",\n",
        "                    line=dict(color='#e74c3c' if t['direction'] == 'GROWING' else '#3498db' if t['direction'] == 'DECLINING' else '#95a5a6',\n",
        "                              width=2, dash='dash'),\n",
        "                ))\n",
        "\n",
        "        fig_daily.add_hline(y=80, line_dash=\"dash\", line_color=\"orange\", annotation_text=\"80% Target\")\n",
        "        fig_daily.add_hline(y=100, line_dash=\"solid\", line_color=\"red\", annotation_text=\"100% Capacity\")\n",
        "        y_upper = max(110, float(daily_summary['TotalUtilPct_mean'].max()) * 1.2)\n",
        "        fig_daily.update_layout(\n",
        "            title=\"Daily Capacity Utilisation (lighter bars = weekends)\",\n",
        "            barmode='stack', xaxis_title=\"Date\", yaxis_title=\"Utilisation %\",\n",
        "            yaxis=dict(range=[0, y_upper]),\n",
        "            height=420, template=\"plotly_white\",\n",
        "            legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"center\", x=0.5)\n",
        "        )\n",
        "        charts['fig_daily'] = _apply_chart_spacing(fig_daily)\n",
        "\n",
        "    # 4. Throttling Timeline\n",
        "    if len(daily_summary) > 0:\n",
        "        fig_throttle = go.Figure()\n",
        "        fig_throttle.add_trace(go.Scatter(x=pd.to_datetime(daily_summary['Date']), y=daily_summary['InteractiveDelayPct_max'], name='Interactive Delay %', mode='lines+markers', line=dict(color='#fd7e14', width=2), fill='tozeroy', fillcolor='rgba(253, 126, 20, 0.2)'))\n",
        "        fig_throttle.add_trace(go.Scatter(x=pd.to_datetime(daily_summary['Date']), y=daily_summary['InteractiveRejectPct_max'], name='Interactive Rejection %', mode='lines+markers', line=dict(color='#dc3545', width=2)))\n",
        "        fig_throttle.add_trace(go.Scatter(x=pd.to_datetime(daily_summary['Date']), y=daily_summary['BackgroundRejectPct_max'], name='Background Rejection %', mode='lines+markers', line=dict(color='#6c757d', width=2, dash='dot')))\n",
        "        fig_throttle.update_layout(title=\"Throttling Analysis Over Time\", xaxis_title=\"Date\", yaxis_title=\"Throttling %\", height=350, template=\"plotly_white\",\n",
        "            legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"center\", x=0.5))\n",
        "        charts['fig_throttle'] = _apply_chart_spacing(fig_throttle)\n",
        "\n",
        "    # 5. Carryforward Analysis\n",
        "    if len(daily_summary) > 0:\n",
        "        fig_carry = make_subplots(rows=1, cols=2, subplot_titles=['Cumulative Carryforward %', 'Expected Burndown Time'])\n",
        "        fig_carry.add_trace(go.Bar(x=pd.to_datetime(daily_summary['Date']).dt.strftime('%d %b'), y=daily_summary['CarryoverCumulativePct_max'],\n",
        "            marker_color=['#dc3545' if v > 50 else '#fd7e14' if v > 20 else '#28a745' for v in daily_summary['CarryoverCumulativePct_max']], name='Carryforward %'), row=1, col=1)\n",
        "        fig_carry.add_trace(go.Scatter(x=pd.to_datetime(daily_summary['Date']), y=daily_summary['ExpectedBurndownMin_max'], mode='lines+markers', line=dict(color='#667eea', width=2), name='Burndown (min)'), row=1, col=2)\n",
        "        fig_carry.update_layout(height=350, showlegend=False, template=\"plotly_white\", title_text=\"Carryforward / Overage Analysis\")\n",
        "        fig_carry.update_xaxes(tickformat=\"%d %b\", row=1, col=1)\n",
        "        fig_carry.update_xaxes(tickformat=\"%d %b\", row=1, col=2)\n",
        "        charts['fig_carryover'] = _apply_chart_spacing(fig_carry)\n",
        "\n",
        "    # 6. SKU Comparison (now includes Reserved pricing)\n",
        "    sku_names = [f\"{s['SKU']}   \" for s in sku_analysis]\n",
        "    sku_utils = [s['Avg Util %'] for s in sku_analysis]\n",
        "    sku_colors = ['#dc3545' if s['Status'] in ['THROTTLING RISK', 'TOO SMALL'] else '#fd7e14' if s['Status'] == 'TIGHT' else '#28a745' if s['Status'] == 'GOOD FIT' else '#667eea' if s['Status'] == 'COMFORTABLE' else '#6c757d' for s in sku_analysis]\n",
        "\n",
        "    fig_sku = go.Figure(go.Bar(y=sku_names, x=[min(u, 100) for u in sku_utils], orientation='h', marker_color=sku_colors,\n",
        "        text=[f\"{u:.0f}%\" + (\" (over)\" if u > 100 else \"\") for u in sku_utils], textposition='outside', cliponaxis=False))\n",
        "    rec_idx = [i for i, s in enumerate(sku_analysis) if s['SKU'] == recommended_sku['name']][0]\n",
        "    rec_util = min(sku_utils[rec_idx], 100)\n",
        "    fig_sku.add_annotation(x=rec_util, y=sku_names[rec_idx], ax=50, ay=0, text=\"<b>RECOMMENDED</b>\", showarrow=True, arrowhead=2, arrowsize=1.2, arrowwidth=2,\n",
        "        arrowcolor='#28a745', font=dict(color='#28a745', size=11), xanchor='left', bgcolor='rgba(255,255,255,0.8)', bordercolor='#28a745', borderwidth=1, borderpad=4)\n",
        "    fig_sku.add_vline(x=80, line_dash=\"dash\", line_color=\"green\", annotation_text=\"80% Target\")\n",
        "    fig_sku.add_vline(x=100, line_dash=\"solid\", line_color=\"red\", annotation_text=\"100% Limit\")\n",
        "    fig_sku.update_layout(title=\"SKU Comparison (Average Utilisation)\", xaxis_title=\"Utilisation %\", xaxis=dict(range=[0, 130]), height=450, margin=dict(l=120, r=80), template=\"plotly_white\")\n",
        "    charts['fig_sku'] = _apply_chart_spacing(fig_sku)\n",
        "\n",
        "    # 7. Top Workloads\n",
        "    if len(df_items_cap) > 0:\n",
        "        name_cols = [c for c in df_items_cap.columns if 'item' in c.lower() and 'name' in c.lower()]\n",
        "        id_cols = [c for c in df_items_cap.columns if 'Item' in c and ('Id' in c or 'Kind' in c)]\n",
        "        cu_cols = [c for c in df_items_cap.columns if 'CU' in c.upper() and 'Total' in c]\n",
        "        label_col = name_cols[0] if name_cols else (id_cols[0] if id_cols else None)\n",
        "\n",
        "        if label_col and cu_cols:\n",
        "            cu_col = cu_cols[0]\n",
        "            item_totals = df_items_cap.groupby(label_col)[cu_col].sum().reset_index()\n",
        "            item_totals = item_totals.sort_values(cu_col, ascending=True).tail(10)\n",
        "            fig_items = go.Figure(go.Bar(y=item_totals[label_col], x=item_totals[cu_col], orientation='h', marker_color='#667eea',\n",
        "                text=[f\"{v/1e6:,.1f}M CUs\" if v >= 1e6 else f\"{v/1e3:,.0f}K CUs\" if v >= 1e3 else f\"{v:,.0f} CUs\" for v in item_totals[cu_col]], textposition='outside', cliponaxis=False))\n",
        "            max_items_x = float(item_totals[cu_col].max()) if len(item_totals) > 0 else 0\n",
        "            fig_items.update_layout(title=\"Top 10 Workloads by CU Consumption\", xaxis_title=\"Total CUs\", xaxis=dict(range=[0, max_items_x * 1.2 if max_items_x > 0 else 1]), height=420, margin=dict(l=280, r=180), template=\"plotly_white\")\n",
        "            charts['fig_items'] = _apply_chart_spacing(fig_items)\n",
        "\n",
        "    # 8. Weekday vs Weekend comparison (if split is enabled)\n",
        "    if WEEKDAY_WEEKEND_SPLIT and len(daily_summary) > 0:\n",
        "        wd = daily_summary[daily_summary['IsWeekday']]\n",
        "        we = daily_summary[~daily_summary['IsWeekday']]\n",
        "        if len(wd) > 0 and len(we) > 0:\n",
        "            categories = ['Avg CUs', 'P80 CUs', 'Avg Util %']\n",
        "            wd_vals = [metrics['weekday_avg_cus'], metrics['weekday_p80_cus'], metrics['weekday_avg_util']]\n",
        "            we_vals = [metrics['weekend_avg_cus'], metrics['weekend_p80_cus'], metrics['weekend_avg_util']]\n",
        "\n",
        "            fig_wdwe = make_subplots(rows=1, cols=3, subplot_titles=categories)\n",
        "            for i, cat in enumerate(categories):\n",
        "                fig_wdwe.add_trace(go.Bar(x=['Weekday', 'Weekend'], y=[wd_vals[i], we_vals[i]],\n",
        "                    marker_color=['#667eea', '#b0c4de'],\n",
        "                    text=[f\"{wd_vals[i]:,.0f}\", f\"{we_vals[i]:,.0f}\"] if i < 2 else [f\"{wd_vals[i]:.1f}%\", f\"{we_vals[i]:.1f}%\"],\n",
        "                    textposition='outside', showlegend=False, cliponaxis=False\n",
        "                ), row=1, col=i+1)\n",
        "                _max_col = max(wd_vals[i], we_vals[i])\n",
        "                fig_wdwe.update_yaxes(range=[0, _max_col * 1.2 if _max_col > 0 else 1], row=1, col=i+1)\n",
        "            fig_wdwe.update_layout(title=\"Weekday vs Weekend Comparison\", height=360, template=\"plotly_white\")\n",
        "            charts['fig_weekday_weekend'] = _apply_chart_spacing(fig_wdwe)\n",
        "\n",
        "    # 9. Workspace Breakdown (if data available)\n",
        "    ws_breakdown = get_workspace_breakdown(df_items_cap)\n",
        "    if len(ws_breakdown) > 0:\n",
        "        top_ws = ws_breakdown.head(10)\n",
        "        fig_ws = go.Figure(go.Bar(\n",
        "            y=top_ws['Workspace'], x=top_ws['Total CUs'], orientation='h',\n",
        "            marker_color='#764ba2',\n",
        "            text=[f\"{s:.0f}%\" for s in top_ws['Share %']],\n",
        "            textposition='outside',\n",
        "            cliponaxis=False\n",
        "        ))\n",
        "        max_ws_x = float(top_ws['Total CUs'].max()) if len(top_ws) > 0 else 0\n",
        "        fig_ws.update_layout(\n",
        "            title=\"Top Workspaces by CU Consumption\",\n",
        "            xaxis_title=\"Total CUs\", xaxis=dict(range=[0, max_ws_x * 1.2 if max_ws_x > 0 else 1]),\n",
        "            height=max(320, len(top_ws) * 40 + 120),\n",
        "            margin=dict(l=280, r=130), template=\"plotly_white\"\n",
        "        )\n",
        "        charts['fig_workspace'] = _apply_chart_spacing(fig_ws)\n",
        "\n",
        "    return charts\n",
        "\n",
        "\n",
        "print(\"[OK] Chart functions defined\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# Analysis Functions: Summary, HTML Report, Lakehouse Export\n",
        "# =============================================================================\n",
        "\n",
        "DISCLAIMER_TEXT = (\n",
        "    \"DISCLAIMER: This analysis is based on data from the Capacity Metrics semantic model \"\n",
        "    \"over the configured time period. Recommendations are indicative, not prescriptive. \"\n",
        "    \"This tool can make mistakes. Key limitations: (1) Spark Autoscale workloads billed \"\n",
        "    \"separately are NOT included in these metrics. (2) Pricing shown uses published PAYG/reserved \"\n",
        "    \"list prices (USD) which vary by region, currency, and agreement. (3) The analysis window is \"\n",
        "    \"limited to the last 14 days of Capacity Metrics retention. (4) If Capacity Overage or Surge \"\n",
        "    \"Protection is enabled, throttling metrics may not reflect true overuse. \"\n",
        "    \"Always validate recommendations against your own workload knowledge and business context \"\n",
        "    \"before making capacity changes.\"\n",
        ")\n",
        "\n",
        "def print_capacity_summary(capacity_id, metrics, current_sku, recommended_sku, sku_analysis, daily_summary=None, capacity_name=None):\n",
        "    \"\"\"Print formatted summary for a single capacity.\"\"\"\n",
        "    m = metrics\n",
        "    print(f\"\"\"\n",
        "{'='*70}\n",
        "FABRIC SKU ADVISOR - ANALYSIS: {capacity_name or capacity_id[:8]}\n",
        "{'='*70}\n",
        "\n",
        "CAPACITY HEALTH SCORE: {m['health_score']}/100 ({m['health_rating']})\n",
        "{'='*50}\n",
        "\n",
        "UTILISATION SUMMARY\n",
        "  Days Analyzed:        {m['days_analyzed']}\n",
        "  Average Daily CUs:    {m['avg_daily_cus']:,.0f}\n",
        "  Peak Daily CUs:       {m['max_daily_cus']:,.0f}\n",
        "  80th Percentile:      {m['p80_daily_cus']:,.0f}\n",
        "\n",
        "  Avg Utilisation:      {m['avg_util']:.1f}%\n",
        "  Peak Utilisation:     {m['max_util']:.1f}%\n",
        "  Interactive:          {m['avg_interactive']:.1f}%\n",
        "  Background:           {m['avg_background']:.1f}%\n",
        "\"\"\")\n",
        "\n",
        "    # Weekday vs Weekend\n",
        "    if WEEKDAY_WEEKEND_SPLIT:\n",
        "        print(f\"\"\"WEEKDAY vs WEEKEND\n",
        "  Weekday Avg CUs:      {m['weekday_avg_cus']:,.0f}  (P80: {m['weekday_p80_cus']:,.0f})\n",
        "  Weekend Avg CUs:      {m['weekend_avg_cus']:,.0f}  (P80: {m['weekend_p80_cus']:,.0f})\n",
        "  Weekday Avg Util:     {m['weekday_avg_util']:.1f}%\n",
        "  Weekend Avg Util:     {m['weekend_avg_util']:.1f}%\n",
        "  Weekday/Weekend Ratio:{m['weekday_weekend_ratio']}x\n",
        "  SKU sized on:         Weekday P80 (filtered)\n",
        "\"\"\")\n",
        "\n",
        "    # Spike filtering\n",
        "    if SPIKE_FILTERING and m.get('spike_days_detected', 0) > 0:\n",
        "        print(f\"\"\"SPIKE FILTERING\n",
        "  Suspected spikes:     {m['spike_days_detected']} day(s) flagged as pause/settlement catch-up\n",
        "  P80 (all days):       {m['p80_daily_cus']:,.0f} CUs\n",
        "  P80 (filtered):       {m['p80_daily_cus_filtered']:,.0f} CUs\n",
        "\"\"\")\n",
        "\n",
        "    # Trend\n",
        "    if TREND_ANALYSIS and m.get('trend', {}).get('has_trend'):\n",
        "        t = m['trend']\n",
        "        print(f\"\"\"CONSUMPTION TREND\n",
        "  Direction:            {t['direction']}\n",
        "  Weekly Growth:        {t['weekly_growth_pct']:+.1f}%\n",
        "  Confidence (R2):      {t['r_squared']}\n",
        "\"\"\")\n",
        "        if t['direction'] == 'GROWING' and t['weekly_growth_pct'] > 5:\n",
        "            print(\"  WARNING: Consumption is growing significantly. Review capacity plan.\")\n",
        "\n",
        "    print(f\"\"\"THROTTLING ASSESSMENT\n",
        "  Days with delays:     {m['days_with_delay']}/{m['days_analyzed']}\n",
        "  Days with rejections: {m['days_with_rejection']}/{m['days_analyzed']}\n",
        "  Avg delay %:          {m['avg_delay_pct']:.2f}%\n",
        "  Max delay %:          {m['max_delay_pct']:.2f}%\n",
        "\n",
        "CARRYFORWARD/OVERAGE\n",
        "  Days with overage:    {m['days_with_carryover']}/{m['days_analyzed']}\n",
        "  Avg carryover %:      {m['avg_carryover_pct']:.2f}%\n",
        "  Max carryover %:      {m['max_carryover_pct']:.2f}%\n",
        "  Avg burndown time:    {format_duration(m['avg_burndown_min'])}\n",
        "  Max burndown time:    {format_duration(m['max_burndown_min'])}\n",
        "\n",
        "SKU RECOMMENDATION\n",
        "  Current SKU:          {current_sku['name'] if current_sku else 'Unknown'}\n",
        "  Recommended SKU:      {recommended_sku['name']}\n",
        "  Daily CU Budget:      {recommended_sku['budget_30s'] * 2880:,} CUs\n",
        "  Expected Utilisation: {calculate_utilisation(m['avg_daily_cus'], recommended_sku)*100:.1f}%\n",
        "\"\"\")\n",
        "\n",
        "    # Action recommendation\n",
        "    if current_sku:\n",
        "        cur_idx = next((i for i, s in enumerate(SKUS) if s['name'] == current_sku['name']), -1)\n",
        "        rec_idx = next((i for i, s in enumerate(SKUS) if s['name'] == recommended_sku['name']), -1)\n",
        "        if rec_idx > cur_idx:\n",
        "            print(f\"  ACTION: Upgrade from {current_sku['name']} to {recommended_sku['name']}\")\n",
        "        elif rec_idx < cur_idx:\n",
        "            print(f\"  ACTION: Consider downsizing from {current_sku['name']} to {recommended_sku['name']} to save costs\")\n",
        "        else:\n",
        "            print(f\"  ACTION: Stay on {current_sku['name']} \\u2013 it is the recommended fit\")\n",
        "\n",
        "    # Cost comparison: PAYG and Reserved\n",
        "    if current_sku and recommended_sku:\n",
        "        cur_payg = current_sku.get(\"monthly_usd\", 0)\n",
        "        rec_payg = recommended_sku.get(\"monthly_usd\", 0)\n",
        "        rec_reserved = recommended_sku.get(\"monthly_reserved_usd\", 0)\n",
        "        diff = cur_payg - rec_payg\n",
        "\n",
        "        print(f\"\\nCOST COMPARISON (published list prices, USD)\")\n",
        "        print(f\"  Current ({current_sku['name']}) PAYG:       ${cur_payg:,}/mo\")\n",
        "        print(f\"  Recommended ({recommended_sku['name']}) PAYG:  ${rec_payg:,}/mo\")\n",
        "        print(f\"  Recommended ({recommended_sku['name']}) Reserved (1yr): ${rec_reserved:,}/mo\")\n",
        "\n",
        "        if RESERVED_VS_PAYG:\n",
        "            saving_vs_payg = rec_payg - rec_reserved\n",
        "            print(f\"\\n  RESERVED vs PAYG ANALYSIS\")\n",
        "            print(f\"    PAYG to Reserved savings:  ${saving_vs_payg:,}/mo (~{RESERVED_DISCOUNT_PCT}%)\")\n",
        "            avg_util_pct = m['avg_util']\n",
        "            if avg_util_pct > RESERVED_BREAKEVEN_UTIL * 100:\n",
        "                print(f\"    Your avg utilisation ({avg_util_pct:.0f}%) is ABOVE the {RESERVED_BREAKEVEN_UTIL*100:.0f}% break-even.\")\n",
        "                print(f\"    Recommendation: Reserved instance likely offers better value.\")\n",
        "            else:\n",
        "                print(f\"    Your avg utilisation ({avg_util_pct:.0f}%) is BELOW the {RESERVED_BREAKEVEN_UTIL*100:.0f}% break-even.\")\n",
        "                print(f\"    Recommendation: PAYG with pause/resume scheduling may be cheaper.\")\n",
        "\n",
        "        if diff > 0:\n",
        "            print(f\"\\n  ESTIMATED SAVINGS vs current: ~${diff:,}/mo (PAYG list prices)\")\n",
        "        elif diff < 0:\n",
        "            print(f\"\\n  ESTIMATED UPGRADE COST: ~${abs(diff):,}/mo (PAYG list prices)\")\n",
        "        print(\"  Note: Actual costs depend on your region, currency, and agreement.\")\n",
        "\n",
        "    # Surge protection note\n",
        "    if m['avg_util'] > 100 and m['days_with_delay'] == 0:\n",
        "        print(f\"\\n  NOTE: Utilisation exceeds 100% but no throttling detected.\")\n",
        "        print(f\"  This may indicate Capacity Overage or Surge Protection is enabled.\")\n",
        "        print(f\"  Check Azure billing for additional consumption costs.\")\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"  {DISCLAIMER_TEXT}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    print(\"\\nFull SKU Comparison:\")\n",
        "    display(pd.DataFrame(sku_analysis))\n",
        "\n",
        "\n",
        "def save_capacity_to_lakehouse(capacity_id, html_content, daily_summary, sku_analysis, metrics, recommended_sku, current_sku, html_filename):\n",
        "    \"\"\"Save per-capacity outputs to Lakehouse.\n",
        "\n",
        "    Strategy:\n",
        "    1) Use attached default Lakehouse when available (/lakehouse/default/Files)\n",
        "    2) Otherwise get/create LAKEHOUSE_NAME (default: LH_Capacity_Advisor) and write via ABFS path\n",
        "    3) File naming follows REPLACE_EXISTING_OUTPUTS policy\n",
        "    \"\"\"\n",
        "    cap_short = capacity_id[:8]\n",
        "    run_suffix = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    try:\n",
        "        import io\n",
        "        try:\n",
        "            import notebookutils\n",
        "            has_notebookutils = True\n",
        "            try:\n",
        "                from notebookutils import mssparkutils\n",
        "                has_mssparkutils = True\n",
        "            except Exception:\n",
        "                mssparkutils = None\n",
        "                has_mssparkutils = False\n",
        "        except ImportError:\n",
        "            has_notebookutils = False\n",
        "            has_mssparkutils = False\n",
        "            notebookutils = None\n",
        "            mssparkutils = None\n",
        "\n",
        "        if not has_notebookutils:\n",
        "            print(\"  [WARN] notebookutils not available. Cannot save to Lakehouse.\")\n",
        "            return\n",
        "\n",
        "        workspace_id = fabric.get_notebook_workspace_id()\n",
        "        default_lakehouse_id = notebookutils.runtime.context.get('defaultLakehouseId')\n",
        "        default_lakehouse_name = notebookutils.runtime.context.get('defaultLakehouseName')\n",
        "\n",
        "        target_lakehouse_name = str(LAKEHOUSE_NAME).strip() if LAKEHOUSE_NAME else \"LH_Capacity_Advisor\"\n",
        "        use_default_mount = False\n",
        "\n",
        "        if default_lakehouse_id:\n",
        "            # Best case: notebook already attached to a default lakehouse.\n",
        "            lakehouse_path = \"/lakehouse/default/Files\"\n",
        "            use_default_mount = True\n",
        "            print(f\"  [INFO] Using attached default Lakehouse: {default_lakehouse_name or default_lakehouse_id[:8]}\")\n",
        "        else:\n",
        "            # No default lakehouse attached: resolve/create target and write through ABFS.\n",
        "            if not has_mssparkutils:\n",
        "                print(\"  [WARN] No default Lakehouse and mssparkutils unavailable; cannot create/select Lakehouse.\")\n",
        "                return\n",
        "\n",
        "            lakehouse_obj = None\n",
        "            try:\n",
        "                lakehouse_obj = mssparkutils.lakehouse.get(target_lakehouse_name)\n",
        "                print(f\"  [INFO] Using existing Lakehouse: {target_lakehouse_name}\")\n",
        "            except Exception:\n",
        "                try:\n",
        "                    print(f\"  [INFO] Lakehouse '{target_lakehouse_name}' not found. Creating...\")\n",
        "                    mssparkutils.lakehouse.create(\n",
        "                        name=target_lakehouse_name,\n",
        "                        description=\"Auto-created by Fabric SKU Advisor for report outputs\",\n",
        "                        workspaceId=workspace_id,\n",
        "                    )\n",
        "                    lakehouse_obj = mssparkutils.lakehouse.get(target_lakehouse_name)\n",
        "                    print(f\"  [SUCCESS] Created Lakehouse: {target_lakehouse_name}\")\n",
        "                except Exception as create_err:\n",
        "                    print(f\"  [ERROR] Could not create/access Lakehouse '{target_lakehouse_name}': {create_err}\")\n",
        "                    return\n",
        "\n",
        "            lakehouse_id = lakehouse_obj.get('id') if isinstance(lakehouse_obj, dict) else None\n",
        "            if not lakehouse_id:\n",
        "                print(\"  [ERROR] Could not resolve Lakehouse ID for ABFS export.\")\n",
        "                return\n",
        "\n",
        "            lakehouse_path = f\"abfss://{workspace_id}@onelake.dfs.fabric.microsoft.com/{lakehouse_id}/Files\"\n",
        "            print(f\"  [INFO] Writing via ABFS path: {lakehouse_path}\")\n",
        "\n",
        "        def _write_text(path: str, text: str):\n",
        "            if use_default_mount:\n",
        "                with open(path, \"w\", encoding=\"utf-8\") as fh:\n",
        "                    fh.write(text)\n",
        "            else:\n",
        "                if has_mssparkutils:\n",
        "                    mssparkutils.fs.put(path, text, True)\n",
        "                else:\n",
        "                    notebookutils.fs.put(path, text, True)\n",
        "\n",
        "        def _write_df_csv(df: pd.DataFrame, path: str, index: bool):\n",
        "            _write_text(path, df.to_csv(index=index))\n",
        "\n",
        "        def _read_csv_if_exists(path: str):\n",
        "            if use_default_mount:\n",
        "                try:\n",
        "                    return pd.read_csv(path)\n",
        "                except FileNotFoundError:\n",
        "                    return None\n",
        "            if has_mssparkutils and mssparkutils.fs.exists(path):\n",
        "                try:\n",
        "                    txt = mssparkutils.fs.head(path, 10 * 1024 * 1024)\n",
        "                    if txt:\n",
        "                        return pd.read_csv(io.StringIO(txt))\n",
        "                except Exception:\n",
        "                    return None\n",
        "            return None\n",
        "\n",
        "        def _build_output_name(prefix: str, ext: str = \"csv\"):\n",
        "            if REPLACE_EXISTING_OUTPUTS:\n",
        "                return f\"{prefix}_{cap_short}.{ext}\"\n",
        "            return f\"{prefix}_{cap_short}_{run_suffix}.{ext}\"\n",
        "\n",
        "        if html_content:\n",
        "            html_path = f\"{lakehouse_path}/{html_filename}\"\n",
        "            _write_text(html_path, html_content)\n",
        "            print(f\"  [SUCCESS] HTML report: {html_filename}\")\n",
        "\n",
        "        if len(daily_summary) > 0:\n",
        "            summary_name = _build_output_name(\"capacity_summary\")\n",
        "            _write_df_csv(daily_summary, f\"{lakehouse_path}/{summary_name}\", index=True)\n",
        "            print(f\"  [SUCCESS] Summary CSV: {summary_name}\")\n",
        "\n",
        "        if sku_analysis:\n",
        "            sku_name = _build_output_name(\"sku_analysis\")\n",
        "            _write_df_csv(pd.DataFrame(sku_analysis), f\"{lakehouse_path}/{sku_name}\", index=False)\n",
        "            print(f\"  [SUCCESS] SKU analysis: {sku_name}\")\n",
        "\n",
        "        # Recommendation summary (for multi-run comparison)\n",
        "        rec_data = {\n",
        "            \"capacity_id\": [capacity_id],\n",
        "            \"generated_at\": [datetime.utcnow().strftime(\"%Y-%m-%d %H:%M UTC\")],\n",
        "            \"current_sku\": [current_sku[\"name\"] if current_sku else \"Unknown\"],\n",
        "            \"recommended_sku\": [recommended_sku[\"name\"]],\n",
        "            \"health_score\": [metrics['health_score']],\n",
        "            \"health_rating\": [metrics['health_rating']],\n",
        "            \"avg_daily_cus\": [metrics['avg_daily_cus']],\n",
        "            \"p80_daily_cus\": [metrics['p80_daily_cus']],\n",
        "            \"avg_utilisation_pct\": [metrics['avg_util']],\n",
        "            \"weekday_avg_cus\": [metrics.get('weekday_avg_cus', 0)],\n",
        "            \"weekend_avg_cus\": [metrics.get('weekend_avg_cus', 0)],\n",
        "            \"trend_direction\": [metrics.get('trend', {}).get('direction', 'N/A')],\n",
        "            \"trend_weekly_growth_pct\": [metrics.get('trend', {}).get('weekly_growth_pct', 0)],\n",
        "            \"spike_days_filtered\": [metrics.get('spike_days_detected', 0)],\n",
        "        }\n",
        "        rec_df = pd.DataFrame(rec_data)\n",
        "\n",
        "        rec_name = _build_output_name(\"recommendation\")\n",
        "        _write_df_csv(rec_df, f\"{lakehouse_path}/{rec_name}\", index=False)\n",
        "        print(f\"  [SUCCESS] Recommendation: {rec_name}\")\n",
        "\n",
        "        history_path = f\"{lakehouse_path}/run_history.csv\"\n",
        "        existing = _read_csv_if_exists(history_path)\n",
        "        updated = pd.concat([existing, rec_df], ignore_index=True) if existing is not None else rec_df\n",
        "        _write_df_csv(updated, history_path, index=False)\n",
        "        print(\"  [SUCCESS] Run history updated: run_history.csv\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  [ERROR] Lakehouse export failed: {e}\")\n",
        "\n",
        "\n",
        "print(\"[OK] Summary and export functions defined\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# Run Analysis for All Capacities\n",
        "# =============================================================================\n",
        "# Single capacity:  full inline display (charts, summary, HTML preview)\n",
        "# Multiple capacities: compact progress per capacity, HTML file per capacity,\n",
        "#                       cross-capacity summary table at the end (Cell 26)\n",
        "#\n",
        "from datetime import datetime\n",
        "from plotly.io import to_html as plotly_to_html\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "all_capacity_results = {}\n",
        "is_multi = ANALYSIS_MODE.strip().lower() == 'multi'\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"MULTI-CAPACITY ANALYSIS: {len(CAPACITY_ID_LIST)} capacit{'y' if len(CAPACITY_ID_LIST) == 1 else 'ies'}\")\n",
        "if is_multi:\n",
        "    print(f\"Mode: MULTI - one HTML report per capacity, compact notebook output.\")\n",
        "else:\n",
        "    print(f\"Mode: SINGLE - full charts and summary displayed inline.\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "for cap_num, cap_id in enumerate(CAPACITY_ID_LIST, 1):\n",
        "    _line = '\\u2500' * 70\n",
        "    print(f\"\\n{_line}\")\n",
        "    cap_name = CAPACITY_NAMES.get(cap_id, cap_id[:8])\n",
        "    print(f\"\\u25B6 Capacity {cap_num}/{len(CAPACITY_ID_LIST)}: {cap_name} ({cap_id[:8]}...)\")\n",
        "    print(f\"{_line}\")\n",
        "\n",
        "    # --- 1. Process timepoints ---\n",
        "    daily_summary, current_sku = process_timepoints_for_capacity(df_timepoints, cap_id)\n",
        "\n",
        "    # If timepoints didn't reveal the SKU, use the Capacities table lookup\n",
        "    if current_sku is None and 'CAPACITY_SKUS' in dir():\n",
        "        _sku_name = CAPACITY_SKUS.get(cap_id, '')\n",
        "        if _sku_name:\n",
        "            current_sku = next((s for s in SKUS if s['name'] == _sku_name), None)\n",
        "\n",
        "    if len(daily_summary) == 0:\n",
        "        print(f\"  [WARNING] No timepoint data for {cap_id}, skipping...\")\n",
        "        continue\n",
        "    print(f\"  [OK] Processed {len(daily_summary)} days of timepoint data\")\n",
        "    if current_sku:\n",
        "        print(f\"  [INFO] Current SKU: {current_sku['name']}\")\n",
        "\n",
        "    # --- 2. Calculate metrics ---\n",
        "    metrics = calculate_capacity_metrics(daily_summary)\n",
        "    if metrics is None:\n",
        "        print(f\"  [WARNING] Could not calculate metrics for {cap_id}, skipping...\")\n",
        "        continue\n",
        "    print(f\"  [OK] Health score: {metrics['health_score']}/100 ({metrics['health_rating']})\")\n",
        "    if WEEKDAY_WEEKEND_SPLIT:\n",
        "        print(f\"  [INFO] Weekday avg: {metrics['weekday_avg_cus']:,.0f} CUs | Weekend avg: {metrics['weekend_avg_cus']:,.0f} CUs\")\n",
        "    if metrics.get('spike_days_detected', 0) > 0:\n",
        "        print(f\"  [INFO] {metrics['spike_days_detected']} suspected spike day(s) detected and filtered\")\n",
        "    if metrics.get('trend', {}).get('has_trend'):\n",
        "        t = metrics['trend']\n",
        "        print(f\"  [INFO] Trend: {t['direction']} ({t['weekly_growth_pct']:+.1f}%/week)\")\n",
        "\n",
        "    # --- 3. SKU recommendation ---\n",
        "    sku_analysis, recommended_sku = recommend_sku_for_capacity(metrics, SKUS, NEEDS_FREE_VIEWERS)\n",
        "    print(f\"  [OK] Recommended SKU: {recommended_sku['name']}\")\n",
        "\n",
        "    # --- 4. Filter items for this capacity ---\n",
        "    df_items_cap = filter_items_for_capacity(df_items, cap_id)\n",
        "\n",
        "    # --- 5. Create charts ---\n",
        "    charts = create_capacity_charts(daily_summary, metrics, sku_analysis, recommended_sku, current_sku, df_items_cap)\n",
        "    print(f\"  [OK] Created {len(charts)} charts\")\n",
        "\n",
        "    # --- 6. Inline display (single capacity only) ---\n",
        "    if not is_multi:\n",
        "        for fig_name, fig in charts.items():\n",
        "            fig.show()\n",
        "        print_capacity_summary(cap_id, metrics, current_sku, recommended_sku, sku_analysis, daily_summary, cap_name)\n",
        "    else:\n",
        "        # Compact one-line summary for multi-capacity mode\n",
        "        _action = \"\"\n",
        "        if current_sku:\n",
        "            cur_idx = next((i for i, s in enumerate(SKUS) if s['name'] == current_sku['name']), -1)\n",
        "            rec_idx = next((i for i, s in enumerate(SKUS) if s['name'] == recommended_sku['name']), -1)\n",
        "            if rec_idx > cur_idx:\n",
        "                _action = f\"Upgrade {current_sku['name']} -> {recommended_sku['name']}\"\n",
        "            elif rec_idx < cur_idx:\n",
        "                _action = f\"Downsize {current_sku['name']} -> {recommended_sku['name']}\"\n",
        "            else:\n",
        "                _action = f\"Stay on {current_sku['name']}\"\n",
        "        print(f\"  [RESULT] {cap_name}: {metrics['health_rating']} ({metrics['health_score']}/100) | Avg util: {metrics['avg_util']:.1f}% | {_action}\")\n",
        "\n",
        "    # --- 7. HTML report (always generated) ---\n",
        "    html_content = None\n",
        "    html_filename = None\n",
        "    if SAVE_HTML_REPORT:\n",
        "        _safe_name = \"\".join(c if c.isalnum() or c in \"-_\" else \"_\" for c in CAPACITY_NAMES.get(cap_id, cap_id[:8]))\n",
        "        if REPLACE_EXISTING_OUTPUTS:\n",
        "            html_filename = f\"sku_advisor_{_safe_name}.html\"\n",
        "        else:\n",
        "            _run_suffix = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            html_filename = f\"sku_advisor_{_safe_name}_{_run_suffix}.html\"\n",
        "        try:\n",
        "            _cap_display = CAPACITY_NAMES.get(cap_id, cap_id[:8])\n",
        "            _cur_sku_name = current_sku[\"name\"] if current_sku and isinstance(current_sku, dict) else \"Unknown\"\n",
        "            _dates = pd.to_datetime(daily_summary[\"Date\"])\n",
        "            _date_range = f\"{_dates.min().strftime('%d %b %Y')} to {_dates.max().strftime('%d %b %Y')}\"\n",
        "\n",
        "            html_parts = []\n",
        "            # --- HTML header + CSS ---\n",
        "            html_parts.append(\"\"\"<!DOCTYPE html>\n",
        "<html lang=\"en\"><head><meta charset=\"UTF-8\"><meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
        "<title>Fabric Capacity Health Report | Data Nova</title>\n",
        "<style>\n",
        "*{margin:0;padding:0;box-sizing:border-box}\n",
        "body{font-family:'Segoe UI',Arial,sans-serif;background:linear-gradient(135deg,#f5f7fa 0%,#e4e8ec 100%);min-height:100vh;color:#333;line-height:1.6;padding:24px}\n",
        ".container{max-width:1200px;margin:0 auto}\n",
        ".header{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:white;padding:40px;border-radius:16px;margin-bottom:24px;box-shadow:0 10px 40px rgba(102,126,234,0.3)}\n",
        ".header h1{font-size:2.5em;font-weight:700;margin-bottom:8px}\n",
        ".header .subtitle{opacity:0.9;font-size:1.1em}\n",
        ".header .meta{margin-top:16px;font-size:0.9em;opacity:0.8}\n",
        ".section{background:white;border-radius:16px;padding:24px;margin-bottom:24px;box-shadow:0 2px 12px rgba(0,0,0,0.08)}\n",
        ".section h2{font-size:1.2em;color:#333;margin-bottom:16px;padding-bottom:12px;border-bottom:2px solid #667eea}\n",
        ".kpi-grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(200px,1fr));gap:16px}\n",
        ".kpi{background:white;border-radius:12px;padding:24px;text-align:center;box-shadow:0 2px 12px rgba(0,0,0,0.08)}\n",
        ".kpi .value{font-size:2.2em;font-weight:700;color:#667eea;margin-bottom:4px}\n",
        ".kpi .label{color:#666;font-size:0.9em;font-weight:500}\n",
        ".chart-wrap{margin:16px 0}\n",
        ".section-header{display:flex;align-items:center;justify-content:space-between;gap:12px;margin-bottom:16px;padding-bottom:12px;border-bottom:2px solid #667eea;flex-wrap:wrap}\n",
        ".section-header h2{font-size:1.2em;color:#333;margin:0;padding-bottom:0;border-bottom:none}\n",
        ".rec-card{background:#f8f9fa;border-radius:12px;padding:20px;border-left:4px solid #667eea}\n",
        ".rec-card p{margin-bottom:8px}\n",
        ".rec-card ul{padding-left:24px;margin-bottom:12px}\n",
        ".rec-card li{margin-bottom:4px}\n",
        ".disclaimer{background:#fff3cd;border-radius:12px;padding:16px;margin-top:16px;border-left:4px solid #ffc107;font-size:0.85em;color:#856404}\n",
        ".insight-box{background:#e8f4f8;border-radius:12px;padding:16px;margin:12px 0;border-left:4px solid #17a2b8}\n",
        ".cost-table{width:100%;border-collapse:collapse;margin:12px 0}\n",
        ".cost-table th,.cost-table td{padding:10px 16px;text-align:left;border-bottom:1px solid #eee}\n",
        ".cost-table th{background:#f8f9fa;font-weight:600;color:#333}\n",
        ".cost-table .highlight{background:#e8f5e9;font-weight:600}\n",
        ".footer{text-align:center;padding:32px;color:#666;font-size:0.9em}\n",
        ".help-btn{background:#f0f4ff;border:1px solid #667eea;color:#667eea;width:28px;height:28px;border-radius:50%;cursor:pointer;font-size:14px;font-weight:600;display:flex;align-items:center;justify-content:center;flex-shrink:0}\n",
        ".help-btn:hover{background:#667eea;color:white}\n",
        ".about-link-btn{background:#f0f4ff;border:1px solid #667eea;color:#667eea;padding:8px 14px;border-radius:999px;cursor:pointer;font-size:13px;font-weight:600;display:inline-flex;align-items:center;justify-content:center;text-decoration:none;line-height:1;min-height:34px}\n",
        ".about-link-btn:hover{background:#667eea;color:white}\n",
        ".modal-overlay{display:none;position:fixed;top:0;left:0;width:100%;height:100%;background:rgba(0,0,0,0.5);z-index:1000;justify-content:center;align-items:center}\n",
        ".modal-overlay.active{display:flex}\n",
        ".modal{background:white;border-radius:16px;padding:32px;max-width:500px;width:90%;max-height:80vh;overflow-y:auto;position:relative;box-shadow:0 20px 60px rgba(0,0,0,0.3)}\n",
        ".modal h4{color:#667eea;margin-bottom:16px;font-size:1.3em}\n",
        ".modal p,.modal ul,.modal li{color:#555;line-height:1.7;margin-bottom:8px;padding-left:20px}\n",
        ".modal-close{position:absolute;top:16px;right:16px;background:#f0f0f0;border:none;width:32px;height:32px;border-radius:50%;cursor:pointer;font-size:18px;color:#666;display:flex;align-items:center;justify-content:center}\n",
        "</style></head><body><div class=\"container\">\n",
        "<div class=\"header\">\n",
        "  <h1>Fabric Capacity Health Report</h1>\n",
        "  <p class=\"subtitle\">Capacity analysis report</p>\n",
        "  <p class=\"meta\" style=\"margin-bottom:8px;\"><strong>Capacity:</strong> \"\"\" + _cap_display + \"\"\" (\"\"\" + _cur_sku_name + \"\"\") &bull; <strong>Period:</strong> \"\"\" + _date_range + \"\"\"</p>\n",
        "  <p class=\"meta\">Prathy Kamasani | Data Nova &mdash; Generated \"\"\" + datetime.utcnow().strftime(\"%Y-%m-%d %H:%M UTC\") + \"\"\"</p>\n",
        "</div>\"\"\")\n",
        "\n",
        "            # --- Disclaimer banner at top ---\n",
        "            html_parts.append('<div class=\"disclaimer\"><strong>Important:</strong> ' + DISCLAIMER_TEXT + '</div>')\n",
        "\n",
        "            # --- KPIs ---\n",
        "            kpi_lines = []\n",
        "            kpi_lines.append(f'<div class=\"kpi\"><div class=\"value\">{metrics[\"health_score\"]}%</div><span class=\"label\">Health Score \\u00b7 {metrics[\"health_rating\"]}</span></div>')\n",
        "\n",
        "            if current_sku and recommended_sku:\n",
        "                _c_idx = next((i for i, s in enumerate(SKUS) if s['name'] == current_sku['name']), -1)\n",
        "                _r_idx = next((i for i, s in enumerate(SKUS) if s['name'] == recommended_sku['name']), -1)\n",
        "                _arrow = '&uarr;' if _r_idx > _c_idx else ('&darr;' if _r_idx < _c_idx else '&check;')\n",
        "                _word = 'Upgrade' if _r_idx > _c_idx else ('Downsize' if _r_idx < _c_idx else 'Stay')\n",
        "                kpi_lines.append(f'<div class=\"kpi\"><div class=\"value\">{current_sku[\"name\"]} {_arrow} {recommended_sku[\"name\"]}</div><span class=\"label\">{_word} \\u00b7 SKU Recommendation</span></div>')\n",
        "            elif recommended_sku:\n",
        "                kpi_lines.append(f'<div class=\"kpi\"><div class=\"value\">{recommended_sku[\"name\"]}</div><span class=\"label\">Recommended SKU</span></div>')\n",
        "\n",
        "            if current_sku and recommended_sku:\n",
        "                _diff = current_sku.get(\"monthly_usd\", 0) - recommended_sku.get(\"monthly_usd\", 0)\n",
        "                if _diff > 0:\n",
        "                    kpi_lines.append(f'<div class=\"kpi\"><div class=\"value\" style=\"color:#28a745;\">~${_diff:,}/mo</div><span class=\"label\">Est. Savings (list price)</span></div>')\n",
        "                elif _diff < 0:\n",
        "                    kpi_lines.append(f'<div class=\"kpi\"><div class=\"value\" style=\"color:#fd7e14;\">~+${abs(_diff):,}/mo</div><span class=\"label\">Est. Upgrade Cost (list price)</span></div>')\n",
        "\n",
        "            kpi_lines.append(f'<div class=\"kpi\"><span class=\"label\">Days Analyzed</span><div class=\"value\">{metrics[\"days_analyzed\"]}</div></div>')\n",
        "            kpi_lines.append(f'<div class=\"kpi\"><span class=\"label\">Avg Daily CUs</span><div class=\"value\">{metrics[\"avg_daily_cus\"]:,.0f}</div></div>')\n",
        "            kpi_lines.append(f'<div class=\"kpi\"><span class=\"label\">Avg Util %</span><div class=\"value\">{metrics[\"avg_util\"]:.1f}%</div></div>')\n",
        "            if metrics['days_with_delay'] > 0:\n",
        "                kpi_lines.append(f'<div class=\"kpi\"><span class=\"label\">Days with Throttling</span><div class=\"value\">{metrics[\"days_with_delay\"]}/{metrics[\"days_analyzed\"]}</div></div>')\n",
        "\n",
        "            # Trend KPI\n",
        "            _trend = metrics.get('trend', {})\n",
        "            if _trend.get('has_trend'):\n",
        "                _t_color = '#dc3545' if _trend['direction'] == 'GROWING' else '#28a745' if _trend['direction'] == 'DECLINING' else '#6c757d'\n",
        "                kpi_lines.append(f'<div class=\"kpi\"><div class=\"value\" style=\"color:{_t_color};\">{_trend[\"weekly_growth_pct\"]:+.1f}%/wk</div><span class=\"label\">Consumption Trend \\u00b7 {_trend[\"direction\"]}</span></div>')\n",
        "\n",
        "            html_parts.append('<div class=\"section\"><div class=\"section-header\"><h2>Key Metrics</h2><button class=\"help-btn\" onclick=\"showHelp(\\'metrics\\')\" title=\"Help\">?</button></div><div class=\"kpi-grid\">' + \"\".join(kpi_lines) + \"</div></div>\")\n",
        "\n",
        "            # --- Weekday vs Weekend insight ---\n",
        "            if WEEKDAY_WEEKEND_SPLIT and metrics.get('weekday_count', 0) > 0 and metrics.get('weekend_count', 0) > 0:\n",
        "                _ratio = metrics['weekday_weekend_ratio']\n",
        "                _insight = f\"Weekday consumption is {_ratio}x higher than weekends. \" if _ratio > 1.5 else \"Weekday and weekend consumption are similar. \"\n",
        "                _insight += f\"SKU recommendation is based on weekday P80 ({metrics['weekday_p80_cus']:,.0f} CUs) for accurate working-day sizing.\"\n",
        "                html_parts.append(f'<div class=\"section\"><div class=\"section-header\"><h2>Weekday vs Weekend</h2><button class=\"help-btn\" onclick=\"showHelp(\\'weekday\\')\" title=\"Help\">?</button></div>'\n",
        "                    f'<div class=\"insight-box\"><strong>Insight:</strong> {_insight}</div>'\n",
        "                    f'<div class=\"kpi-grid\">'\n",
        "                    f'<div class=\"kpi\"><div class=\"value\">{metrics[\"weekday_avg_cus\"]:,.0f}</div><span class=\"label\">Weekday Avg CUs</span></div>'\n",
        "                    f'<div class=\"kpi\"><div class=\"value\">{metrics[\"weekend_avg_cus\"]:,.0f}</div><span class=\"label\">Weekend Avg CUs</span></div>'\n",
        "                    f'<div class=\"kpi\"><div class=\"value\">{metrics[\"weekday_avg_util\"]:.1f}%</div><span class=\"label\">Weekday Avg Util</span></div>'\n",
        "                    f'<div class=\"kpi\"><div class=\"value\">{metrics[\"weekend_avg_util\"]:.1f}%</div><span class=\"label\">Weekend Avg Util</span></div>'\n",
        "                    f'</div></div>')\n",
        "\n",
        "            # --- Spike filtering note ---\n",
        "            if SPIKE_FILTERING and metrics.get('spike_days_detected', 0) > 0:\n",
        "                _n = metrics['spike_days_detected']\n",
        "                html_parts.append(f'<div class=\"section\"><div class=\"section-header\"><h2>Spike Filtering</h2><button class=\"help-btn\" onclick=\"showHelp(\\'spike\\')\" title=\"Help\">?</button></div>'\n",
        "                    f'<div class=\"insight-box\"><strong>Note:</strong> {_n} day(s) showed anomalously high peak CUs, '\n",
        "                    f'likely from capacity pause/resume or settlement catch-up processing. '\n",
        "                    f'These were excluded from the P80 calculation used for SKU sizing. '\n",
        "                    f'P80 (all days): {metrics[\"p80_daily_cus\"]:,.0f} CUs vs P80 (filtered): {metrics[\"p80_daily_cus_filtered\"]:,.0f} CUs.</div></div>')\n",
        "\n",
        "            # --- Charts ---\n",
        "            chart_list = [\n",
        "                (\"fig_util_gauges\", f\"Utilisation on {recommended_sku['name']}\", \"util\"),\n",
        "                (\"fig_daily\", \"Daily Utilisation (lighter bars = weekends)\", \"daily\"),\n",
        "                (\"fig_weekday_weekend\", \"Weekday vs Weekend Comparison\", \"weekday-chart\"),\n",
        "                (\"fig_throttle\", \"Throttling (delay & rejection %)\", \"throttle\"),\n",
        "                (\"fig_carryover\", \"Carryforward & Expected Burndown\", \"carryover\"),\n",
        "                (\"fig_sku\", \"SKU Utilisation Comparison\", \"sku\"),\n",
        "                (\"fig_items\", \"Top Workloads by CUs\", \"items\"),\n",
        "                (\"fig_workspace\", \"Top Workspaces by CU Consumption\", \"workspace\"),\n",
        "            ]\n",
        "            for fig_name, title, help_id in chart_list:\n",
        "                fig = charts.get(fig_name)\n",
        "                if fig is not None and hasattr(fig, \"to_html\"):\n",
        "                    html_parts.append(f'<div class=\"section\"><div class=\"section-header\"><h2>{title}</h2><button class=\"help-btn\" onclick=\"showHelp(\\'{help_id}\\')\" title=\"Help\">?</button></div><div class=\"chart-wrap\">')\n",
        "                    html_parts.append(fig.to_html(full_html=False, include_plotlyjs=\"cdn\"))\n",
        "                    html_parts.append(\"</div></div>\")\n",
        "\n",
        "            # --- Reserved vs PAYG comparison ---\n",
        "            if RESERVED_VS_PAYG and recommended_sku:\n",
        "                rec_payg = recommended_sku.get(\"monthly_usd\", 0)\n",
        "                rec_reserved = recommended_sku.get(\"monthly_reserved_usd\", 0)\n",
        "                rec_saving = rec_payg - rec_reserved\n",
        "                avg_util_decimal = metrics['avg_util'] / 100.0\n",
        "\n",
        "                _payg_advice = \"\"\n",
        "                if avg_util_decimal > RESERVED_BREAKEVEN_UTIL:\n",
        "                    _payg_advice = f'<div class=\"insight-box\" style=\"background:#e8f5e9;border-color:#28a745;\"><strong>Recommendation:</strong> Your average utilisation ({metrics[\"avg_util\"]:.0f}%) is above the ~{RESERVED_BREAKEVEN_UTIL*100:.0f}% break-even point. A <strong>reserved instance</strong> would likely save ~${rec_saving:,}/mo vs PAYG for {recommended_sku[\"name\"]}.</div>'\n",
        "                else:\n",
        "                    _payg_advice = f'<div class=\"insight-box\" style=\"background:#fff3cd;border-color:#ffc107;\"><strong>Recommendation:</strong> Your average utilisation ({metrics[\"avg_util\"]:.0f}%) is below the ~{RESERVED_BREAKEVEN_UTIL*100:.0f}% break-even point. <strong>PAYG with pause/resume scheduling</strong> may be more cost-effective than a reserved instance.</div>'\n",
        "\n",
        "                _cost_rows = \"\"\n",
        "                for sa in sku_analysis:\n",
        "                    _hl = ' class=\"highlight\"' if sa['SKU'] == recommended_sku['name'] else ''\n",
        "                    _cost_rows += f'<tr{_hl}><td>{sa[\"SKU\"]}</td><td>{sa[\"Avg Util %\"]}%</td><td>${sa[\"PAYG $/mo\"]:,}</td><td>${sa[\"Reserved $/mo\"]:,}</td><td>${sa[\"Savings $/mo\"]:,}</td></tr>'\n",
        "\n",
        "                html_parts.append(f'<div class=\"section\"><div class=\"section-header\"><h2>Reserved vs PAYG Cost Comparison</h2><button class=\"help-btn\" onclick=\"showHelp(\\'reserved\\')\" title=\"Help\">?</button></div>'\n",
        "                    f'{_payg_advice}'\n",
        "                    f'<table class=\"cost-table\"><thead><tr><th>SKU</th><th>Avg Util %</th><th>PAYG $/mo</th><th>Reserved $/mo</th><th>Savings $/mo</th></tr></thead>'\n",
        "                    f'<tbody>{_cost_rows}</tbody></table>'\n",
        "                    f'<p style=\"font-size:0.85em;color:#666;margin-top:8px;\">Published list prices (USD). Reserved = 1-year commitment. Actual costs vary by region, currency, and agreement. Break-even at ~{RESERVED_BREAKEVEN_UTIL*100:.0f}% utilisation.</p></div>')\n",
        "\n",
        "            # --- Recommendations ---\n",
        "            rec_html = '<div class=\"section\"><div class=\"section-header\"><h2>Recommendations</h2><button class=\"help-btn\" onclick=\"showHelp(\\'rec\\')\" title=\"Help\">?</button></div><div class=\"rec-card\">'\n",
        "            _h = metrics['health_rating']\n",
        "            _rn = recommended_sku['name']\n",
        "            if _h == 'CRITICAL':\n",
        "                rec_html += f'<p style=\"color:#dc3545;font-weight:600;\">URGENT: Critically overloaded.</p><ul><li>Upgrade to <strong>{_rn}</strong> immediately</li><li>Reschedule heavy background jobs</li><li>Check if job-level bursting is enabled (default: ON). Disable if single jobs are monopolising capacity.</li></ul>'\n",
        "            elif _h == 'POOR':\n",
        "                rec_html += f'<p style=\"color:#fd7e14;font-weight:600;\">WARNING: Significant stress.</p><ul><li>Plan upgrade to <strong>{_rn}</strong></li><li>Spread workloads across off-peak hours</li><li>Review job-level bursting settings if concurrency is a concern</li></ul>'\n",
        "            elif _h == 'FAIR':\n",
        "                rec_html += f'<p style=\"color:#856404;font-weight:600;\">ATTENTION: Running warm.</p><ul><li>Consider <strong>{_rn}</strong></li><li>Optimise heavy refresh schedules</li></ul>'\n",
        "            elif _h == 'GOOD':\n",
        "                rec_html += '<p style=\"color:#28a745;font-weight:600;\">HEALTHY: Well-sized.</p><ul><li>Continue monitoring for growth trends</li></ul>'\n",
        "            else:\n",
        "                rec_html += '<p style=\"color:#28a745;font-weight:600;\">EXCELLENT: Plenty of headroom.</p><ul><li>Consider downsizing to save costs</li></ul>'\n",
        "\n",
        "            # Trend warning\n",
        "            _trend = metrics.get('trend', {})\n",
        "            if _trend.get('has_trend') and _trend['direction'] == 'GROWING' and _trend['weekly_growth_pct'] > 3:\n",
        "                rec_html += f'<li style=\"color:#dc3545;\">Consumption is growing at {_trend[\"weekly_growth_pct\"]:+.1f}%/week. Plan for the next SKU tier within the coming weeks.</li>'\n",
        "\n",
        "            # Surge/overage note\n",
        "            if metrics['avg_util'] > 100 and metrics['days_with_delay'] == 0:\n",
        "                rec_html += '<li>No throttling detected despite high utilisation. Check if Capacity Overage or Surge Protection is enabled, as these incur additional Azure charges.</li>'\n",
        "\n",
        "            rec_html += '</ul></div>'\n",
        "\n",
        "            # Spark autoscale disclaimer\n",
        "            rec_html += '<div class=\"insight-box\"><strong>Spark Autoscale Note:</strong> If Spark Autoscale Billing is enabled for this capacity, Spark workloads are billed separately on a pay-as-you-go basis and are NOT reflected in these metrics. Factor in those costs separately when evaluating total Fabric spend.</div>'\n",
        "\n",
        "            rec_html += '</div>'\n",
        "            html_parts.append(rec_html)\n",
        "\n",
        "            # --- Help modals ---\n",
        "            html_parts.append('''<div class=\"modal-overlay\" id=\"modal-metrics\" onclick=\"closeModal(event, 'metrics')\"><div class=\"modal\" onclick=\"event.stopPropagation()\"><button class=\"modal-close\" onclick=\"closeModal(event, 'metrics')\">×</button><h4>Understanding Key Metrics</h4><p>These metrics summarise your Fabric capacity health:</p><ul><li><strong>Health Score:</strong> Composite 0–100 score from utilisation, throttling and carryforward.</li><li><strong>SKU Recommendation:</strong> Upgrade / downsize / stay based on usage.</li><li><strong>Avg Daily CUs, Avg Util %, Days with Throttling:</strong> Core consumption and stress signals.</li><li><strong>Consumption Trend:</strong> Week-over-week growth when available.</li></ul></div></div>''')\n",
        "            html_parts.append('''<div class=\"modal-overlay\" id=\"modal-weekday\" onclick=\"closeModal(event, 'weekday')\"><div class=\"modal\" onclick=\"event.stopPropagation()\"><button class=\"modal-close\" onclick=\"closeModal(event, 'weekday')\">×</button><h4>Weekday vs Weekend</h4><p>Capacity Metrics can split consumption by weekday vs weekend. SKU recommendation uses weekday P80 for sizing so working-day demand is reflected. If weekend usage is much lower, PAYG with pause/resume may be cost-effective.</p></div></div>''')\n",
        "            html_parts.append('''<div class=\"modal-overlay\" id=\"modal-spike\" onclick=\"closeModal(event, 'spike')\"><div class=\"modal\" onclick=\"event.stopPropagation()\"><button class=\"modal-close\" onclick=\"closeModal(event, 'spike')\">×</button><h4>Spike Filtering</h4><p>Days with anomalously high peak CUs (e.g. after capacity pause/resume or settlement catch-up) can be excluded from the P80 used for SKU sizing. This avoids one-off spikes from driving an oversized recommendation.</p></div></div>''')\n",
        "            html_parts.append('''<div class=\"modal-overlay\" id=\"modal-util\" onclick=\"closeModal(event, 'util')\"><div class=\"modal\" onclick=\"event.stopPropagation()\"><button class=\"modal-close\" onclick=\"closeModal(event, 'util')\">×</button><h4>Utilisation Gauges</h4><p>Shows average and peak utilisation on the recommended SKU. Target is typically 60–80% average with peak below 95% to avoid throttling. Green = good fit; red = risk.</p></div></div>''')\n",
        "            html_parts.append('''<div class=\"modal-overlay\" id=\"modal-daily\" onclick=\"closeModal(event, 'daily')\"><div class=\"modal\" onclick=\"event.stopPropagation()\"><button class=\"modal-close\" onclick=\"closeModal(event, 'daily')\">×</button><h4>Daily Utilisation</h4><p>Daily consumption over time. Lighter bars = weekends. Use this to see spikes, weekday/weekend patterns and outliers.</p></div></div>''')\n",
        "            html_parts.append('''<div class=\"modal-overlay\" id=\"modal-weekday-chart\" onclick=\"closeModal(event, 'weekday-chart')\"><div class=\"modal\" onclick=\"event.stopPropagation()\"><button class=\"modal-close\" onclick=\"closeModal(event, 'weekday-chart')\">×</button><h4>Weekday vs Weekend Comparison</h4><p>Compares average consumption on weekdays vs weekends. Helps decide if pause/resume or reserved capacity is better.</p></div></div>''')\n",
        "            html_parts.append('''<div class=\"modal-overlay\" id=\"modal-throttle\" onclick=\"closeModal(event, 'throttle')\"><div class=\"modal\" onclick=\"event.stopPropagation()\"><button class=\"modal-close\" onclick=\"closeModal(event, 'throttle')\">×</button><h4>Throttling (Delay & Rejection %)</h4><p>When capacity is overloaded, Fabric delays or rejects operations. This chart shows interactive delay %, interactive rejection % and background rejection % over time. High or rising values mean the capacity is undersized.</p></div></div>''')\n",
        "            html_parts.append('''<div class=\"modal-overlay\" id=\"modal-carryover\" onclick=\"closeModal(event, 'carryover')\"><div class=\"modal\" onclick=\"event.stopPropagation()\"><button class=\"modal-close\" onclick=\"closeModal(event, 'carryover')\">×</button><h4>Carryforward & Expected Burndown</h4><p>Carryforward is accumulated capacity debt when consumption exceeds the SKU. Burndown is how long it takes to clear that debt. High carryforward or long burndown indicates sustained overuse.</p></div></div>''')\n",
        "            html_parts.append('''<div class=\"modal-overlay\" id=\"modal-sku\" onclick=\"closeModal(event, 'sku')\"><div class=\"modal\" onclick=\"event.stopPropagation()\"><button class=\"modal-close\" onclick=\"closeModal(event, 'sku')\">×</button><h4>SKU Utilisation Comparison</h4><p>Compares your workload against all Fabric SKUs. Bar length = average utilisation %. Aim for the recommended SKU in the green zone (around 80% target).</p></div></div>''')\n",
        "            html_parts.append('''<div class=\"modal-overlay\" id=\"modal-items\" onclick=\"closeModal(event, 'items')\"><div class=\"modal\" onclick=\"event.stopPropagation()\"><button class=\"modal-close\" onclick=\"closeModal(event, 'items')\">×</button><h4>Top Workloads by CUs</h4><p>Which reports, semantic models, dataflows, etc. consume the most capacity. Use this to target optimisation (schedule, incremental refresh, or right-size).</p></div></div>''')\n",
        "            html_parts.append('''<div class=\"modal-overlay\" id=\"modal-workspace\" onclick=\"closeModal(event, 'workspace')\"><div class=\"modal\" onclick=\"event.stopPropagation()\"><button class=\"modal-close\" onclick=\"closeModal(event, 'workspace')\">×</button><h4>Top Workspaces by CU Consumption</h4><p>Which workspaces use the most capacity. Helps with chargeback or prioritising which areas to optimise.</p></div></div>''')\n",
        "            html_parts.append('''<div class=\"modal-overlay\" id=\"modal-reserved\" onclick=\"closeModal(event, 'reserved')\"><div class=\"modal\" onclick=\"event.stopPropagation()\"><button class=\"modal-close\" onclick=\"closeModal(event, 'reserved')\">×</button><h4>Reserved vs PAYG</h4><p>Reserved capacity (1-year commitment) has a lower effective price at high utilisation. The table shows PAYG vs reserved list prices. Above the break-even utilisation (~60–70%), reserved usually saves money; below it, PAYG with pause/resume may be cheaper.</p></div></div>''')\n",
        "            html_parts.append('''<div class=\"modal-overlay\" id=\"modal-rec\" onclick=\"closeModal(event, 'rec')\"><div class=\"modal\" onclick=\"event.stopPropagation()\"><button class=\"modal-close\" onclick=\"closeModal(event, 'rec')\">×</button><h4>Recommendations</h4><p>Actionable advice based on health rating (CRITICAL/POOR/FAIR/GOOD/EXCELLENT). Includes upgrade/downsize guidance, throttling and scheduling tips, and notes on trend and Spark Autoscale.</p></div></div>''')\n",
        "            html_parts.append('''<div class=\"modal-overlay\" id=\"modal-overview\" onclick=\"closeModal(event, 'overview')\"><div class=\"modal\" onclick=\"event.stopPropagation()\"><button class=\"modal-close\" onclick=\"closeModal(event, 'overview')\">×</button><h4>About This Report</h4><p>This Fabric Capacity Health Report is generated from the Capacity Metrics semantic model. It includes health score, SKU recommendation, throttling and carryforward analysis, and cost comparison. Click the <strong>?</strong> next to each section for more detail.</p><p>Data Nova | Microsoft Fabric Training &amp; Consulting</p></div></div>''')\n",
        "\n",
        "            # --- Footer with disclaimer ---\n",
        "            html_parts.append(f'<div class=\"section disclaimer\"><strong>Disclaimer:</strong> {DISCLAIMER_TEXT}</div>')\n",
        "            html_parts.append('<div class=\"footer\"><p>Fabric SKU Advisor \\u2013 Advanced | <a href=\"https://www.data-nova.io\">Data Nova</a></p><p><button class=\"about-link-btn\" onclick=\"showHelp(\\'overview\\')\" style=\"margin-top:8px\">About this report</button></p></div></div><script>function showHelp(type){document.getElementById(\"modal-\"+type).classList.add(\"active\");document.body.style.overflow=\"hidden\"}function closeModal(event,type){document.getElementById(\"modal-\"+type).classList.remove(\"active\");document.body.style.overflow=\"\"}document.addEventListener(\"keydown\",function(e){if(e.key===\"Escape\"){document.querySelectorAll(\".modal-overlay\").forEach(function(m){m.classList.remove(\"active\")});document.body.style.overflow=\"\"}});</script></body></html>')\n",
        "\n",
        "            html_content = \"\\n\".join(html_parts)\n",
        "\n",
        "            # HTML report generated as a file, not rendered inline\n",
        "            # (inline would duplicate the charts already shown via fig.show())\n",
        "            display(HTML(f\"<p style='color:#28a745;font-weight:bold;'>\\u2713 HTML report generated: {html_filename}</p>\"))\n",
        "\n",
        "            # Save HTML to Lakehouse if enabled\n",
        "            if SAVE_TO_LAKEHOUSE:\n",
        "                try:\n",
        "                    lh_path = f\"/lakehouse/default/Files/{html_filename}\"\n",
        "                    with open(lh_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                        f.write(html_content)\n",
        "                    print(f\"  [SUCCESS] HTML report saved to Lakehouse: {html_filename}\")\n",
        "                except Exception:\n",
        "                    print(f\"  [WARNING] Could not save to attached Lakehouse.\")\n",
        "            else:\n",
        "                print(f\"  [INFO] Set SAVE_TO_LAKEHOUSE = True to persist HTML reports.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  [ERROR] HTML report generation failed: {e}\")\n",
        "\n",
        "    # --- 8. Lakehouse export ---\n",
        "    if SAVE_TO_LAKEHOUSE:\n",
        "        save_capacity_to_lakehouse(cap_id, html_content, daily_summary, sku_analysis, metrics, recommended_sku, current_sku, html_filename)\n",
        "\n",
        "    # --- 9. Store results ---\n",
        "    all_capacity_results[cap_id] = {\n",
        "        'daily_summary': daily_summary,\n",
        "        'current_sku': current_sku,\n",
        "        'recommended_sku': recommended_sku,\n",
        "        'metrics': metrics,\n",
        "        'sku_analysis': sku_analysis,\n",
        "        'charts': charts,\n",
        "        'html_content': html_content,\n",
        "        'html_filename': html_filename if SAVE_HTML_REPORT else None,\n",
        "    }\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"ANALYSIS COMPLETE: {len(all_capacity_results)}/{len(CAPACITY_ID_LIST)} capacities analyzed\")\n",
        "if is_multi and SAVE_HTML_REPORT:\n",
        "    print(f\"\\nHTML reports generated:\")\n",
        "    for cid, r in all_capacity_results.items():\n",
        "        if r.get('html_filename'):\n",
        "            print(f\"  {r['html_filename']}\")\n",
        "    if not SAVE_TO_LAKEHOUSE:\n",
        "        print(f\"\\nTo save these reports, set SAVE_TO_LAKEHOUSE = True and re-run.\")\n",
        "print(f\"{'='*70}\")\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# Cross-Capacity Summary\n",
        "# =============================================================================\n",
        "from IPython.display import display, HTML\n",
        "from datetime import datetime\n",
        "\n",
        "if len(all_capacity_results) > 1:\n",
        "\n",
        "    # Build summary data\n",
        "    rows = []\n",
        "    for cap_id, r in all_capacity_results.items():\n",
        "        m = r['metrics']\n",
        "        cur = r['current_sku']\n",
        "        rec = r['recommended_sku']\n",
        "        t = m.get('trend', {})\n",
        "\n",
        "        # Action word + arrow\n",
        "        if cur:\n",
        "            c_idx = next((i for i, s in enumerate(SKUS) if s['name'] == cur['name']), -1)\n",
        "            r_idx = next((i for i, s in enumerate(SKUS) if s['name'] == rec['name']), -1)\n",
        "            if r_idx > c_idx:\n",
        "                action_html = f\"{cur['name']} &rarr; <strong>{rec['name']}</strong>\"\n",
        "                action_class = \"badge-upgrade\"\n",
        "            elif r_idx < c_idx:\n",
        "                action_html = f\"{cur['name']} &rarr; <strong>{rec['name']}</strong>\"\n",
        "                action_class = \"badge-downsize\"\n",
        "            else:\n",
        "                action_html = f\"<strong>{cur['name']}</strong> (stay)\"\n",
        "                action_class = \"badge-stay\"\n",
        "        else:\n",
        "            action_html = f\"<strong>{rec['name']}</strong>\"\n",
        "            action_class = \"badge-stay\"\n",
        "\n",
        "        # Health badge colour\n",
        "        hs = m['health_score']\n",
        "        if hs >= 90:   h_color = '#20c997'\n",
        "        elif hs >= 75: h_color = '#28a745'\n",
        "        elif hs >= 50: h_color = '#ffc107'\n",
        "        elif hs >= 25: h_color = '#fd7e14'\n",
        "        else:          h_color = '#dc3545'\n",
        "\n",
        "        # Trend indicator\n",
        "        trend_html = \"\"\n",
        "        if TREND_ANALYSIS and t.get('has_trend'):\n",
        "            t_icon = \"&#9650;\" if t['direction'] == 'GROWING' else \"&#9660;\" if t['direction'] == 'DECLINING' else \"&#9644;\"\n",
        "            t_color = '#dc3545' if t['direction'] == 'GROWING' else '#28a745' if t['direction'] == 'DECLINING' else '#6c757d'\n",
        "            trend_html = f'<span style=\"color:{t_color};font-weight:600;\">{t_icon} {t[\"weekly_growth_pct\"]:+.1f}%/wk</span>'\n",
        "\n",
        "        # Monthly cost (recommended SKU)\n",
        "        payg = rec.get('monthly_usd', 0)\n",
        "        reserved = rec.get('monthly_reserved_usd', 0)\n",
        "\n",
        "        rows.append({\n",
        "            'cap_id': cap_id,\n",
        "            'cap_short': CAPACITY_NAMES.get(cap_id, cap_id[:8]),\n",
        "            'action_html': action_html,\n",
        "            'action_class': action_class,\n",
        "            'health_score': hs,\n",
        "            'health_rating': m['health_rating'],\n",
        "            'h_color': h_color,\n",
        "            'avg_util': m['avg_util'],\n",
        "            'trend_html': trend_html,\n",
        "            'payg': payg,\n",
        "            'reserved': reserved,\n",
        "            'html_filename': r.get('html_filename', ''),\n",
        "        })\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# Cross-Capacity Comparison Table\n",
        "# =============================================================================\n",
        "from IPython.display import display, HTML\n",
        "from datetime import datetime\n",
        "\n",
        "if len(all_capacity_results) > 1:\n",
        "\n",
        "    # Build styled HTML table\n",
        "    table_rows = \"\"\n",
        "    for row in rows:\n",
        "        avg_util_str = f\"{row['avg_util']:.1f}\"\n",
        "        payg_str = f\"${row['payg']:,}\"\n",
        "        reserved_str = f\"${row['reserved']:,}\"\n",
        "        table_rows += (\n",
        "            '<tr>'\n",
        "            f'<td style=\"font-weight:600;\">{row[\"cap_short\"]}</td>'\n",
        "            f'<td><span class=\"{row[\"action_class\"]}\">{row[\"action_html\"]}</span></td>'\n",
        "            f'<td style=\"text-align:center;\"><span class=\"health-badge\" style=\"background:{row[\"h_color\"]};\">{row[\"health_score\"]}</span> {row[\"health_rating\"]}</td>'\n",
        "            f'<td style=\"text-align:center;\">{avg_util_str}%</td>'\n",
        "            f'<td style=\"text-align:center;\">{row[\"trend_html\"]}</td>'\n",
        "            f'<td style=\"text-align:right;\">{payg_str}</td>'\n",
        "            f'<td style=\"text-align:right;\">{reserved_str}</td>'\n",
        "            '</tr>'\n",
        "        )\n",
        "\n",
        "    # File list\n",
        "    file_list_html = \"\"\n",
        "    for row in rows:\n",
        "        if row['html_filename']:\n",
        "            file_list_html += f'<li><code>{row[\"html_filename\"]}</code></li>'\n",
        "\n",
        "    save_note = \"\"\n",
        "    if not SAVE_TO_LAKEHOUSE:\n",
        "        save_note = '<p style=\"margin-top:8px;color:#856404;\">Set <code>SAVE_TO_LAKEHOUSE = True</code> to persist these reports.</p>'\n",
        "\n",
        "    ts_str = datetime.utcnow().strftime(\"%d %b %Y %H:%M UTC\")\n",
        "    n_caps = len(rows)\n",
        "\n",
        "    summary_html = f\"\"\"\n",
        "    <style>\n",
        "    .xc-table {{font-family:'Segoe UI',Arial,sans-serif;border-collapse:collapse;width:100%;margin:16px 0;font-size:0.95em;}}\n",
        "    .xc-table th {{background:linear-gradient(135deg,#667eea,#764ba2);color:white;padding:12px 16px;text-align:left;font-weight:600;}}\n",
        "    .xc-table td {{padding:10px 16px;border-bottom:1px solid #eee;}}\n",
        "    .xc-table tr:hover {{background:#f8f9fa;}}\n",
        "    .health-badge {{display:inline-block;color:white;font-weight:700;padding:4px 10px;border-radius:12px;font-size:0.85em;min-width:36px;text-align:center;}}\n",
        "    .badge-upgrade {{color:#dc3545;}}\n",
        "    .badge-downsize {{color:#28a745;}}\n",
        "    .badge-stay {{color:#6c757d;}}\n",
        "    .xc-header {{background:linear-gradient(135deg,#667eea 0%,#764ba2 100%);color:white;padding:24px 32px;border-radius:12px 12px 0 0;}}\n",
        "    .xc-header h2 {{margin:0;font-size:1.4em;font-weight:700;}}\n",
        "    .xc-header p {{margin:4px 0 0;opacity:0.85;font-size:0.9em;}}\n",
        "    .xc-wrap {{background:white;border-radius:12px;box-shadow:0 2px 12px rgba(0,0,0,0.08);margin-bottom:20px;overflow:hidden;}}\n",
        "    .xc-footer {{padding:12px 24px;background:#f8f9fa;font-size:0.85em;color:#666;}}\n",
        "    .xc-files {{padding:12px 24px;font-size:0.9em;}}\n",
        "    .xc-files ul {{margin:8px 0 0 20px;}}\n",
        "    .xc-files li {{margin-bottom:4px;}}\n",
        "    </style>\n",
        "\n",
        "    <div class=\"xc-wrap\">\n",
        "      <div class=\"xc-header\">\n",
        "    <h2>Cross-Capacity Comparison</h2>\n",
        "    <p>{n_caps} capacities analysed &bull; {ts_str}</p>\n",
        "      </div>\n",
        "      <table class=\"xc-table\">\n",
        "    <thead>\n",
        "      <tr>\n",
        "        <th>Capacity</th>\n",
        "        <th>SKU Action</th>\n",
        "        <th style=\"text-align:center;\">Health</th>\n",
        "        <th style=\"text-align:center;\">Avg Util</th>\n",
        "        <th style=\"text-align:center;\">Trend</th>\n",
        "        <th style=\"text-align:right;\">PAYG $/mo</th>\n",
        "        <th style=\"text-align:right;\">Reserved $/mo</th>\n",
        "      </tr>\n",
        "    </thead>\n",
        "    <tbody>\n",
        "      {table_rows}\n",
        "    </tbody>\n",
        "      </table>\n",
        "      <div class=\"xc-files\">\n",
        "    <strong>Detailed reports (one per capacity):</strong>\n",
        "    <ul>{file_list_html}</ul>\n",
        "    {save_note}\n",
        "      </div>\n",
        "      <div class=\"xc-footer\">\n",
        "    Pricing: published list prices (USD). Actual costs vary by region, currency, and agreement.\n",
        "    &bull; Prathy Kamasani | <a href=\"https://www.data-nova.io\" style=\"color:#667eea;\">Data Nova</a>\n",
        "      </div>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "    display(HTML(summary_html))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# Save Comparison CSV + Footer\n",
        "# =============================================================================\n",
        "from datetime import datetime\n",
        "\n",
        "if len(all_capacity_results) > 1:\n",
        "\n",
        "    # Save CSV if Lakehouse is enabled\n",
        "    if SAVE_TO_LAKEHOUSE:\n",
        "        try:\n",
        "            csv_rows = []\n",
        "            for cap_id, r in all_capacity_results.items():\n",
        "                m = r['metrics']\n",
        "                t = m.get('trend', {})\n",
        "                csv_rows.append({\n",
        "                    'capacity_id': cap_id,\n",
        "                    'capacity_name': CAPACITY_NAMES.get(cap_id, cap_id[:8]),\n",
        "                    'current_sku': r['current_sku']['name'] if r['current_sku'] else 'Unknown',\n",
        "                    'recommended_sku': r['recommended_sku']['name'],\n",
        "                    'health_score': m['health_score'],\n",
        "                    'health_rating': m['health_rating'],\n",
        "                    'avg_util_pct': round(m['avg_util'], 1),\n",
        "                    'avg_daily_cus': round(m['avg_daily_cus']),\n",
        "                    'weekday_avg_cus': round(m.get('weekday_avg_cus', 0)),\n",
        "                    'weekend_avg_cus': round(m.get('weekend_avg_cus', 0)),\n",
        "                    'trend_direction': t.get('direction', 'N/A'),\n",
        "                    'trend_weekly_pct': t.get('weekly_growth_pct', 0),\n",
        "                    'throttling_days': m['days_with_delay'],\n",
        "                    'payg_usd': r['recommended_sku'].get('monthly_usd', 0),\n",
        "                    'reserved_usd': r['recommended_sku'].get('monthly_reserved_usd', 0),\n",
        "                })\n",
        "            _ts = datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n",
        "            lh_path = f\"/lakehouse/default/Files/capacity_comparison_{_ts}.csv\"\n",
        "            pd.DataFrame(csv_rows).to_csv(lh_path, index=False)\n",
        "            print(f\"[SUCCESS] Comparison CSV saved: capacity_comparison_{_ts}.csv\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] Could not save comparison: {e}\")\n",
        "\n",
        "elif len(all_capacity_results) == 1:\n",
        "    print(\"\\n[INFO] Single capacity analysed. Full report displayed above.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n[WARNING] No capacities were successfully analysed.\")\n",
        "\n",
        "print(f\"\\nBuilt by Prathy Kamasani | Data Nova\")\n",
        "print(\"https://www.data-nova.io\")\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}